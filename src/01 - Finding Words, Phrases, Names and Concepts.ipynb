{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3badbbc-fef5-4ce6-81c3-f8cb16b6c8c2",
   "metadata": {},
   "source": [
    "<center>    \n",
    "    <h1 id='spacy-notebook-1' style='color:#7159c1; font-size:350%'>Finding Words, Phrases, Names and Concepts</h1>\n",
    "    <i style='font-size:125%'>Introduction to Spacy Package</i>\n",
    "</center>\n",
    "\n",
    "> **Topics**\n",
    "\n",
    "```\n",
    "- ‚öôÔ∏è NLP Processes\n",
    "- üìÅ Spacy Documents, Tokens and Spans\n",
    "- üìù Lexical Attributes\n",
    "- ü™à Pipelines\n",
    "- üè∑Ô∏è Named Entities (NER)\n",
    "- üîç Matches\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be3b107-a04b-433a-b19a-16061b711544",
   "metadata": {},
   "source": [
    "<h1 id='0-nlp-processes' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>‚öôÔ∏è | NLP Processes</h1>\n",
    "\n",
    "`Natural Language Processing (NLP)` is a field of Artificial Intelligence and Machine Learning focused on processing sequential texts. It can be used for a variety of tasks, since in `Phonemes` and `Morphemes & Lexemes` to `Syntax` and `Context` tasks. Applications-wise, we can do `Speech to Text`, `Documents Summary`, `Part-of-Speech (POS) Tagging` projects and much more!!\n",
    "\n",
    "The image below shows some of the applications we can do diving into each block of language.\n",
    "\n",
    "<figure style='text-aling:center'>\n",
    "    <img style='border-radius:20px' src='./images/0-nlp-processes.png' alt='Diagram of possible NLP Applications in each Block of Language' />\n",
    "    <figcaption>Figure 1 - Diagram of possible NLP Applications in each Block of Language. By <a href='https://www.oreilly.com/library/view/practical-natural-language/9781492054047/ch01.html'>Oreilly - Practical Natural Language Book - Chapter 1</a>.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b003f556-e9c1-4e29-9e55-f45e20d5837d",
   "metadata": {},
   "source": [
    "<h1 id='1-spacy-documents-tokens-and-spans' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>üìÅ | Spacy Documents, Tokens and Spans</h1>\n",
    "\n",
    "`Spacy` is a great Python package to work with NLP and in this section we are going to dive into its basic objects: Documents, Tokens and Spans! First off, in order to install the package, we should run the following command:\n",
    "\n",
    "```bash\n",
    "pip install -U spacy\n",
    "```\n",
    "\n",
    "In Spacy, we always create a blank Language Model (LM) or load a pre-trained one. For now, we'll be creating a blank one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6136a95-e56f-44a3-a012-a31df253d907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importings\n",
    "import spacy\n",
    "\n",
    "# Creating a blank English NLP Object AKA Processing Pipeline\n",
    "nlp = spacy.blank('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5d4448-cae4-497c-9f08-232803e3075e",
   "metadata": {},
   "source": [
    "After creating a Processing Pipeline, we can process sequential texts to create a Document with Tokens. By default, Spacy works with `n-gram` Tokens, that is, each word is considered as a Token.\n",
    "\n",
    "Besides, when two or more tokens are together, they're called Span, so:\n",
    "\n",
    "- **Document** - `object of Tokens`;\n",
    "- **Token** - `a word or punctuation`;\n",
    "- **Span** - `two or more tokens together`.\n",
    "\n",
    "There are some observations about Tokenization to keep in mind:\n",
    "\n",
    "    1. single spaces are not considered as tokens, but multiple spaces are considered as a unique token;\n",
    "    \n",
    "    2. words with hiphen are split into multiple tokens, for instance, the word 'ad-free' is split into three tokens: ['ad', '-', 'free']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5bc2273-3e31-40cf-858b-14c221c8e6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Full Document Text: Hey it's me, Goku!\n",
      "---\n",
      "- Text of Each Token:\n",
      "Hey\n",
      "it\n",
      "'s\n",
      "me\n",
      ",\n",
      "Goku\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "# Document\n",
    "document = nlp('Hey it\\'s me, Goku!')\n",
    "\n",
    "print(f'- Full Document Text: {document.text}')\n",
    "print('---')\n",
    "\n",
    "print('- Text of Each Token:')\n",
    "for token in document: print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2b1ffcd-10d2-4995-a014-981cf94a8bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Single Token Text: Goku\n"
     ]
    }
   ],
   "source": [
    "# Token\n",
    "token = document[5]\n",
    "print(f'- Single Token Text: {token.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f884083-95e2-4cdc-be66-988542e9bf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Single Span Token Text: Hey it's me\n"
     ]
    }
   ],
   "source": [
    "# Span\n",
    "span = document[0:4]\n",
    "print(f'- Single Span Token Text: {span.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a27e51-d76e-4604-b0bc-8891774a23b2",
   "metadata": {},
   "source": [
    "<h1 id='2-lexical-attributes' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>üìù | Lexical Attributes</h1>\n",
    "\n",
    "`Lexical Attributes` are info about the words from a language that are `context-independent`, that is, they are not influenced neither by the role of the word in a sentence nor by the context of it.\n",
    "\n",
    "There are lot of Lexical Attributes, so let's just keep in mind the main ones by now:\n",
    "\n",
    "- **i** - `token's index in the document`;\n",
    "- **text** - `token's text in string type`;\n",
    "- **is_alpha** - `whether a token contains only alphabetic characters (a-zA-Z)`;\n",
    "- **is_punct** - `whether a token is a punctuation`;\n",
    "- **is_digit** - `whether a token contains only digits (0-9)`;\n",
    "- **like_num** - `whether a token resemble a number (5 or 'five')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd37ff91-3cef-4bfa-946e-12b558f0f07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "- Text: ['It', 'costs', '$', '5.00', '.', 'Yeah', ',', 'five', 'dollars', '!']\n",
      "- Is Alphabetic? [True, True, False, False, False, True, False, True, True, False]\n",
      "- Is Punctuation? [False, False, False, False, True, False, True, False, False, True]\n",
      "- Is Digit? [False, False, False, False, False, False, False, False, False, False]\n",
      "- Is Like a Number? [False, False, False, True, False, False, False, True, False, False]\n"
     ]
    }
   ],
   "source": [
    "# Lexical Attributes\n",
    "document = nlp('It costs $5.00. Yeah, five dollars!')\n",
    "\n",
    "print(f'- Index: {[token.i for token in document]}')\n",
    "print(f'- Text: {[token.text for token in document]}')\n",
    "print(f'- Is Alphabetic? {[token.is_alpha for token in document]}')\n",
    "print(f'- Is Punctuation? {[token.is_punct for token in document]}')\n",
    "print(f'- Is Digit? {[token.is_digit for token in document]}')\n",
    "print(f'- Is Like a Number? {[token.like_num for token in document]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149ca6a4-d617-4c90-bef3-931ce6818c53",
   "metadata": {},
   "source": [
    "<h1 id='3-pipelines' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>ü™à | Pipelines</h1>\n",
    "\n",
    "Only Lexical Attributes cannot be enough in some projects where we have to get information about `Part-of-Speech (POS)`, `Named Entities (NER)` and `Contexts`, that is, `context-dependent` variables.\n",
    "\n",
    "In these scenarios, we must have a pre-trained model that has learnt the corpus and rules of the target language, and in order to achieve it, we can either create a model from the scratch or load a pre-trained one.\n",
    "\n",
    "Since creating a model from the scratch is not the goal of this notebook and I still don't have the enough knowledge for it (üòÇ), we will be working with English pre-trained models from Spacy.\n",
    "\n",
    "---\n",
    "\n",
    "Actually, Spacy contains four Language Models (LM) in English:\n",
    "\n",
    "- **en_core_web_sm** - `small pipeline and doesn't contain WordVectors`;\n",
    "- **en_core_web_md** - `middle pipeline and contains WordVectors`;\n",
    "- **en_core_web_lg** - `large pipeline and contains WordVectors`;\n",
    "- **en_core_web_trf** - `equivalent to the large pipeline but with Transformers rather than WordVectors and more focused on accuracy`.\n",
    "\n",
    "While `sm, md and lg` Pipelines are focused on `Efficiency` and works faster, `trf` Pipeline is focused on `Accuracy` and demands more time and computational cost to process.\n",
    "\n",
    "Since these models are not automatically installed with Spacy, we must to install them manually running the following commands:\n",
    "\n",
    "```bash\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download en_core_web_md\n",
    "python -m spacy download en_core_web_lg\n",
    "python -m spacy download en_core_web_trf\n",
    "```\n",
    "\n",
    "Even though the Pipelines are pre-trained already, we can fine-tune them with our own dataset!\n",
    "\n",
    "---\n",
    "\n",
    "And finally, let's take a look at some of the advantages in using pre-trained Pipelines:\n",
    "\n",
    "- **Part-of-Speech (.pos_) Recognition** - `whether a token is a verb, noun, pronoun...`;\n",
    "- **Dependency Label (.dep_) Recognition** - `what role a token takes in the document accordingly to the context, such as nominal subject, object, root, determinant...`;\n",
    "- **Syntatic Head (.head.text)** - `parent token that is directly related to the current token accordingly to the current token Dependency Label`;\n",
    "- **Stop Word (.is_stop) Detection** - `wheter a token is a Stop Word`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30d5c515-237a-4c45-82cb-e5994ab46e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Large Pipeline\n",
    "nlp_large = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f2c4615-29a0-4ae2-89fc-e9133a347b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Text: Hey\n",
      "- Part-of-Speech (POS): INTJ\n",
      "- Dependency Label: intj\n",
      "- Head Token: 's\n",
      "- Is Stop Word: False\n",
      "---\n",
      "- Text: it\n",
      "- Part-of-Speech (POS): PRON\n",
      "- Dependency Label: nsubj\n",
      "- Head Token: 's\n",
      "- Is Stop Word: True\n",
      "---\n",
      "- Text: 's\n",
      "- Part-of-Speech (POS): AUX\n",
      "- Dependency Label: ROOT\n",
      "- Head Token: 's\n",
      "- Is Stop Word: True\n",
      "---\n",
      "- Text: me\n",
      "- Part-of-Speech (POS): PRON\n",
      "- Dependency Label: attr\n",
      "- Head Token: 's\n",
      "- Is Stop Word: True\n",
      "---\n",
      "- Text: ,\n",
      "- Part-of-Speech (POS): PUNCT\n",
      "- Dependency Label: punct\n",
      "- Head Token: 's\n",
      "- Is Stop Word: False\n",
      "---\n",
      "- Text: Goku\n",
      "- Part-of-Speech (POS): PROPN\n",
      "- Dependency Label: attr\n",
      "- Head Token: 's\n",
      "- Is Stop Word: False\n",
      "---\n",
      "- Text: !\n",
      "- Part-of-Speech (POS): PUNCT\n",
      "- Dependency Label: punct\n",
      "- Head Token: 's\n",
      "- Is Stop Word: False\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Checking Tokens' Context-Dependent Info\n",
    "document = nlp_large('Hey it\\'s me, Goku!')\n",
    "\n",
    "for token in document:\n",
    "    print(f'- Text: {token.text}')\n",
    "    print(f'- Part-of-Speech (POS): {token.pos_}')\n",
    "    print(f'- Dependency Label: {token.dep_}')\n",
    "    print(f'- Head Token: {token.head.text}')\n",
    "    print(f'- Is Stop Word: {token.is_stop}')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31645c25-82ca-4487-a6d6-7d01315838b5",
   "metadata": {},
   "source": [
    "<h1 id='4-named-entities-ner' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>üè∑Ô∏è | Named Entities (NER)</h1>\n",
    "\n",
    "A common task when dealing with sequential texts is to identify real-world object, such as companies, organizations, people, countries, objects, currencies... This kind of objects are called `Entities` or `Named Entities (NER)` in NLP.\n",
    "\n",
    "All recognized entities from a document can be acessed via the following attributes:\n",
    "\n",
    "- **document.ents** - `returns a list of Spans, where each Span corresponds to an entity`;\n",
    "- **Entity Label (.label_)** - `returns what kind of entity the Span is. We can use 'spacy.explain' method to get a further explanation about the label definition`.\n",
    "\n",
    "Since whether a Token or Span is an entity depends on the context, it's also needed to work with a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e567a8d-418b-4a6f-aefe-c31a2bc896f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Entity Text: Apple\n",
      "- Label: ORG\n",
      "- Explanation: Companies, agencies, institutions, etc.\n",
      "---\n",
      "- Entity Text: U.K.\n",
      "- Label: GPE\n",
      "- Explanation: Countries, cities, states\n",
      "---\n",
      "- Entity Text: $1 billion\n",
      "- Label: MONEY\n",
      "- Explanation: Monetary values, including unit\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Listing Entities\n",
    "document = nlp_large('Apple is looking at buying U.K. startup at $1 billion.')\n",
    "\n",
    "for entity in document.ents:\n",
    "    print(f'- Entity Text: {entity.text}')\n",
    "    print(f'- Label: {entity.label_}')\n",
    "    print(f'- Explanation: {spacy.explain(entity.label_)}')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b3bf41f-4788-4f00-946f-bea82399f1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- GPE: Countries, cities, states\n",
      "- NNP: noun, proper singular\n",
      "- dobj: direct object\n"
     ]
    }
   ],
   "source": [
    "# We can also call 'spacy.explain' method to get further info about Spacy's abbreviations\n",
    "print(f'- GPE: {spacy.explain(\"GPE\")}')\n",
    "print(f'- NNP: {spacy.explain(\"NNP\")}')\n",
    "print(f'- dobj: {spacy.explain(\"dobj\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2083be5-d656-45d1-9ef9-bb39e9428f93",
   "metadata": {},
   "source": [
    "However, there are some situations where entities are not identified automatically by Spacy. In these situations, we must get them as Spans manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc5404ce-e819-4c2e-bd5b-6f31a04e2c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Entity Text: Apple\n",
      "- Label: ORG\n",
      "- Explanation: Companies, agencies, institutions, etc.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Getting Entities Manually (1)\n",
    "text = 'Upcoming iPhone X release date leaked as Apple reveals pre-orders'\n",
    "document = nlp_large(text)\n",
    "\n",
    "for entity in document.ents:\n",
    "    print(f'- Entity Text: {entity.text}')\n",
    "    print(f'- Label: {entity.label_}')\n",
    "    print(f'- Explanation: {spacy.explain(entity.label_)}')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68159b54-281c-4a42-9684-9cda5b6416e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Missing Entity: iPhone X\n"
     ]
    }
   ],
   "source": [
    "# Getting Entities Manually (2)\n",
    "iphone_x_entity = document[1:3]\n",
    "print(f'- Missing Entity: {iphone_x_entity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71ad195-2206-468f-b024-8498c6760a27",
   "metadata": {},
   "source": [
    "We will learn better ways to fetch non-recognized entities and even make the model learn to automatically identify them in the further lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a7922f-876b-42e6-bacf-25fa0e820963",
   "metadata": {},
   "source": [
    "<h1 id='5-matches' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>üîç | Matches</h1>\n",
    "\n",
    "Sometimes we desire to apply rules in order to find words or phrases into text and are in these situations that `Matches` and `RegEx` come in action.\n",
    "\n",
    "While `RegEx` searches contents only into strings, `Matches` allow us to search content into Spacy Documents, Tokens and Spans, applying advanced linguistic searches, such as, match the word 'duck' only when it's a noun and not a verb.\n",
    "\n",
    "Matches are passed as a list of dictionaries, where each dictionary corresponds to a single token search rule. Besides, search rules are applied sequentially in the Tokens.\n",
    "\n",
    "Oh, and there are some filter Operators that we can use with Matcher:\n",
    "\n",
    "- **{ 'OP': '!' }** - `Negation - Match 0 times`;\n",
    "- **{ 'OP': '?' }** - `Optional - Match 0 or 1 times`;\n",
    "- **{ 'OP': '+' }** - `Match 1 or more times`;\n",
    "- **{ 'OP': '*' }** - `Match 0 or more times`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6aaea50b-2de9-40d2-bbf8-654ad7c21186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matches (1)\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "pattern1 = [{ 'TEXT': 'iPhone' }, { 'TEXT': 'X' }] # matches a token with text 'iPhone' followed by a token with text 'X'\n",
    "pattern2 = [{ 'LOWER': 'iphone' }, { 'LOWER': 'x' }] # matches a token with lowercase text 'iphone' followed by a token with lowercase text 'x'\n",
    "pattern3 = [{ 'LEMMA': 'buy' }, { 'POS': 'NOUN' }] # matches a token with lemma (dictionary form) as 'buy' followed by a token with part-of-speech (pos) as 'NOUN'. For instance, 'buy flowers' and 'bought books'\n",
    "\n",
    "matcher = Matcher(nlp_large.vocab) # always feed Matcher with Vocab when instantiating it\n",
    "matcher.add('IPHONE_PATTERN_1', [pattern1])\n",
    "matcher.add('IPHONE_PATTERN_2', [pattern2])\n",
    "matcher.add('IPHONE_PATTERN_3', [pattern3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe9e0f7-7c8b-4ae1-924b-59f43384b081",
   "metadata": {},
   "source": [
    "Matcher returns a list of tuples containing the following three elements each:\n",
    "\n",
    "- **match_id** - `match id (a random hash)`;\n",
    "- **start** - `start index of the matched Span into the Document`;\n",
    "- **end** - `end index of the matched Span into the Document`.\n",
    "\n",
    "Also, we can create a Span of the match by slicing the Document by 'start' and 'end' indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39428779-27de-4925-99b2-332d27c16467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Match ID: 7162708591567205619\n",
      "- Matched Span: iPhone X\n",
      "---\n",
      "- Match ID: 16007330697362006008\n",
      "- Matched Span: iPhone X\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "matches = matcher(document)\n",
    "\n",
    "for match_id, start_index, end_index in matches:\n",
    "    matched_span = document[start_index: end_index]\n",
    "\n",
    "    print(f'- Match ID: {match_id}')\n",
    "    print(f'- Matched Span: {matched_span.text}')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333cce5b-5329-4efc-acc7-e6d93614a0f5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's do some exercises with Matches now!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2d12f64-ea94-4649-bb16-f40fb0d03e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Tournament' Name: 2018 FIFA World Cup:\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1) Extract the name of the tournament\n",
    "document = nlp_large('2018 FIFA World Cup: France won!')\n",
    "\n",
    "pattern_fifa = [\n",
    "    { 'IS_DIGIT': True }\n",
    "    , { 'LOWER': 'fifa' }\n",
    "    , { 'LOWER': 'world' }\n",
    "    , { 'LOWER': 'cup' }\n",
    "    , { 'IS_PUNCT': True }\n",
    "]\n",
    "\n",
    "matcher = Matcher(nlp_large.vocab)\n",
    "matcher.add('FIFA_PATTERN', [pattern_fifa])\n",
    "matches = matcher(document)\n",
    "\n",
    "for match_id, start_index, end_index in matches:\n",
    "    matched_span = document[start_index:end_index]\n",
    "    print(f'- Tournament\\' Name: {matched_span.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7f3cea0-15b3-49eb-b953-ddd88c9ba42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Matched Span: loved cats\n",
      "- Matched Span: love dogs\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2) Apply two filters in a single token\n",
    "document = nlp_large('I loved cats but I love dogs more')\n",
    "\n",
    "pattern_animals = [{ 'LEMMA': 'love', 'POS': 'VERB' }, { 'POS': 'NOUN' }]\n",
    "\n",
    "matcher = Matcher(nlp_large.vocab)\n",
    "matcher.add('ANIMALS_SEARCH', [pattern_animals])\n",
    "matches = matcher(document)\n",
    "\n",
    "for match_id, start_index, end_index in matches:\n",
    "    matched_span = document[start_index:end_index]\n",
    "    print(f'- Matched Span: {matched_span.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ec877ce-e58b-4d05-b483-28c7490f9d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Matched Span: bought a smartphone\n",
      "- Matched Span: buying apps\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3) Apply filters using operators\n",
    "document = nlp_large('I bought a smartphone. Now I am buying apps.')\n",
    "\n",
    "pattern_with_operators = [\n",
    "    { 'LEMMA': 'buy' }\n",
    "    , { 'POS': 'DET', 'OP': '?' }\n",
    "    , { 'POS': 'NOUN' }\n",
    "]\n",
    "\n",
    "matcher = Matcher(nlp_large.vocab)\n",
    "matcher.add('PATTERN_WITH_OPERATORS', [pattern_with_operators])\n",
    "matches = matcher(document)\n",
    "\n",
    "for match_id, start_index, end_index in matches:\n",
    "    matched_span = document[start_index:end_index]\n",
    "    print(f'- Matched Span: {matched_span.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9b745e6-1ff8-403b-b5a4-dcd25f079cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Matched Span: love cats\n",
      "- Matched Span: very happy\n",
      "- Matched Span: very very happy\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4) Applu filters using operators again\n",
    "document = nlp_large('I love cats and I am very very happy')\n",
    "\n",
    "pattern_love_cats = [{ 'LEMMA': 'love', 'POS': 'VERB' }, { 'LOWER': 'cats' }]\n",
    "pattern_very_happy = [{ 'LOWER': 'very', 'OP': '+' }, { 'LOWER': 'happy' }]\n",
    "\n",
    "matcher = Matcher(nlp_large.vocab)\n",
    "matcher.add('PATTERN_LOVE_CATS', [pattern_love_cats])\n",
    "matcher.add('PATTERN_VERY_HAPPY', [pattern_very_happy])\n",
    "matches = matcher(document)\n",
    "\n",
    "for match_id, start_index, end_index in matches:\n",
    "    matched_span = document[start_index:end_index]\n",
    "    print(f'- Matched Span: {matched_span.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb759898-edd0-4e78-9ec8-2fc9b2b969b8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h1 id='reach-me' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>üì´ | Reach Me</h1>\n",
    "\n",
    "> **Email** - [csfelix08@gmail.com](mailto:csfelix08@gmail.com?)\n",
    "\n",
    "> **Linkedin** - [linkedin.com/in/csfelix/](https://www.linkedin.com/in/csfelix/)\n",
    "\n",
    "> **GitHub:** - [CSFelix](https://github.com/CSFelix)\n",
    "\n",
    "> **Kaggle** - [DSFelix](https://www.kaggle.com/dsfelix)\n",
    "\n",
    "> **Portfolio** - [CSFelix.io](https://csfelix.github.io/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

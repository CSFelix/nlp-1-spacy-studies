{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90ed98cc-b487-4645-a2b4-4d80aa7b9f10",
   "metadata": {},
   "source": [
    "<center>    \n",
    "    <h1 id='spacy-notebook-2' style='color:#7159c1; font-size:350%'>Data Structures</h1>\n",
    "    <i style='font-size:125%'>Diving into Spacy's Architecture and Objects</i>\n",
    "</center>\n",
    "\n",
    "> **Topics**\n",
    "\n",
    "```\n",
    "- üìñ Vocab, StringStore and Lexeme\n",
    "- üìÅ Documents, Tokens and Spans (Part II)\n",
    "- ‚ú® Part-of-Speech (POS), Morphemes and Sentence Segmentation\n",
    "- ü™û Word Vectors and Semantic Similarity\n",
    "- üé® Combining Predictions and Rules\n",
    "- üîç PhraseMatcher, Morphological Attributes Matcher and DependencyMatcher\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7a6102-bb72-4f38-ae7e-3953b14d5d9d",
   "metadata": {},
   "source": [
    "<h1 id='0-vocab-stringstore-and-lexeme' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>üìñ | Vocab, StringStore and Lexeme</h1>\n",
    "\n",
    "`Vocab`, `StringStore` and `Lexeme`: what are they? What is their relationship? Why are they so important to Spacy's architecture?\n",
    "\n",
    "Before creating Documents with Spacy, our pipeline must know a considerable amount of words from our target language in order to be able to identify the Part-of-Speech (POS), Dependency Label, Syntatic Head and so on of each Token.\n",
    "\n",
    "All words are stored into `StringStore` object and, in order to save memory and avoid duplicated entries, each word is stored only once and then is assigned to a string hash.\n",
    "\n",
    "Consequently, each string hash is stored into `Vocab` object, where each element is called `Lexeme`. Since Vocab only stores words hashes independetly, that is, without context and sequential texts, Lexemes contain only `context-independent` info, such as whether a token is a digit, alphabetic or punctuation character.\n",
    "\n",
    "There's alse the `Document` object, where each element is a `Token` and since Document stores sequential texts with context, each Token contains `context-dependent` info, such as Part-of-Speech (POS), Dependency Label, Flag to Stop Words and Syntatic Head.\n",
    "\n",
    "In a nutshell, `StringStore` stores words as string only once and each word is assigned to a string hash. This very hash is stored into `Vocab` as a `Lexeme` object containing all `context-independent` info of it.\n",
    "\n",
    "Besides, every time we create a `Document` in Spacy, the Document accesses Vocab in order to check the existence of the word. If it exists, Spacy gets the hash and search for the word into StringStore in order to associate it to the Document. If it doesn't exist, the word is inserted right away into Vocab and StringStore, then Spacy associate the word to the Document.\n",
    "\n",
    "So, we can say that each time we search for a word, Spacy looks for its hash value into Vocab and then for its string representation into StringStore.\n",
    "\n",
    "The image below illustrates the association between these objects:\n",
    "\n",
    "<figure style='text-aling:center'>\n",
    "    <img style='border-radius:20px' src='./images/1-spacy-architecture.png' alt='Spacy Architecture to Store Words' />\n",
    "    <figcaption>Figure 1 - Spacy Architecture to Store Words By <a href='https://course.spacy.io/en/chapter2'>Spacy - Advanced NLP with Spacy Course - Chapter 2</a>.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5476664-157e-4bca-ac9c-ac769e35509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blank models are instatiated with empty Vocab and StringStore,\n",
    "# whereas pre-trained model are instantiated with both objects\n",
    "# populated.\n",
    "#\n",
    "# In both models, blank and pre-trained, new words are automatically\n",
    "# inserted into Vocab and StringStore while creating Documents.\n",
    "#\n",
    "import spacy\n",
    "nlp_blank = spacy.blank('en')\n",
    "document = nlp_blank('I love Natural Language Processing!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09249dad-a7b3-4cbe-aba2-8cd3ea838c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Love Hash: 3702023516439754181\n",
      "- Love String: love\n"
     ]
    }
   ],
   "source": [
    "# Getting Hash and String of a word\n",
    "love_hash = nlp_blank.vocab.strings['love']\n",
    "love_string = nlp_blank.vocab.strings[love_hash]\n",
    "\n",
    "print(f'- Love Hash: {love_hash}')\n",
    "print(f'- Love String: {love_string}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2be3101d-ae02-460a-853c-3da64f60af97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Hate Hash: 8706232279129489120\n",
      "- Hate String: hate\n"
     ]
    }
   ],
   "source": [
    "# Adding a new Word into Vocab and StringStore\n",
    "nlp_blank.vocab.strings.add('hate')\n",
    "\n",
    "hate_hash = nlp_blank.vocab.strings['hate']\n",
    "hate_string = nlp_blank.vocab.strings[hate_hash]\n",
    "\n",
    "print(f'- Hate Hash: {hate_hash}')\n",
    "print(f'- Hate String: {hate_string}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e82c2c7-7893-46fc-a9b0-ef8bd7284f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Document Love Hash: 3702023516439754181\n",
      "- Document Love String: love\n"
     ]
    }
   ],
   "source": [
    "# Accessing Directly via Document\n",
    "document_love_hash = document.vocab.strings['love'] # Documents also contains its own 'vocab' and 'StringStore' objects\n",
    "document_love_string = document.vocab.strings[document_love_hash]\n",
    "\n",
    "print(f'- Document Love Hash: {document_love_hash}')\n",
    "print(f'- Document Love String: {document_love_string}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e763dea-08c0-4a29-b8f4-6490567ae3ee",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7fc8118-3bba-42a4-a7d8-1a2e0c0b2704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Text: love\n",
      "- Hash Value: 3702023516439754181\n",
      "- Some Lexical Attributes:\n",
      "\t- Is Alphabetic? True\n",
      "\t- Is Punctuation? False\n",
      "\t- Is Digit? False\n",
      "\t- Is Like a Number? False\n"
     ]
    }
   ],
   "source": [
    "# Exploring Lexemes\n",
    "document = nlp_blank('I love Natural Language Processing!')\n",
    "lexeme = nlp_blank.vocab['love']\n",
    "\n",
    "print(f'- Text: {lexeme.text}')\n",
    "print(f'- Hash Value: {lexeme.orth}')\n",
    "print(f'- Some Lexical Attributes:')\n",
    "print(f'\\t- Is Alphabetic? {lexeme.is_alpha}')\n",
    "print(f'\\t- Is Punctuation? {lexeme.is_punct}')\n",
    "print(f'\\t- Is Digit? {lexeme.is_digit}')\n",
    "print(f'\\t- Is Like a Number? {lexeme.like_num}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7316cef2-24d1-4b09-bd27-40d60ab1d37f",
   "metadata": {},
   "source": [
    "<h1 id='1-documents-tokens-and-spans-part-ii' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>üìÅ | Documents, Tokens and Spans (Part II)</h1>\n",
    "\n",
    "Recapping what we already know about these three objects:\n",
    "\n",
    "- **Documents** - `object of Tokens, that is, a sequential text with a context`;\n",
    "- **Token** - `a word or a punctuation and each element into a Document`;\n",
    "- **Span** - `slices of the Document, consisting in two or more Tokens together`.\n",
    "\n",
    "We have seen that Documents are created automatically when processing texts with `nlp` object like below:\n",
    "\n",
    "```python\n",
    "document = nlp('Hey it is me, Goku!')\n",
    "```\n",
    "\n",
    "However, we can create them manually passing only three parameters:\n",
    "\n",
    "- **Vocab** - `Vocab object of the target language`;\n",
    "- **Words** - `list of the sequential text where each element is a Token`;\n",
    "- **Spaces** - `list of flags telling whether there is a space right after the corresponding word of the same index into 'words' parameter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cbd9678-ccee-4dae-8af6-6406ce3952ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Document Text: Hey it 's me, Goku!\n"
     ]
    }
   ],
   "source": [
    "# Manually Creating a Document\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "nlp_large = spacy.load('en_core_web_lg')\n",
    "\n",
    "words = ['Hey', 'it', '\\'s', 'me', ',', 'Goku', '!']\n",
    "spaces = [True, True, True, False, True, False, False]\n",
    "document = Doc(nlp_large.vocab, words=words, spaces=spaces)\n",
    "\n",
    "print(f'- Document Text: {document}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5672cd65-44c0-4053-ba3c-a18f3addd992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Span Without Label: Goku - (label: )\n",
      "- Span With Label: Goku - (label: PERSON)\n"
     ]
    }
   ],
   "source": [
    "# Manually Creating a Span\n",
    "span_without_label = Span(document, 5, 6)\n",
    "span_with_label = Span(document, 5, 6, label='PERSON')\n",
    "\n",
    "print(f'- Span Without Label: {span_without_label} - (label: {span_without_label.label_})')\n",
    "print(f'- Span With Label: {span_with_label} - (label: {span_with_label.label_})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63f63d0a-f4a7-499d-9432-6cb54b68d9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Document Named Entities (NER) - Before Manual Update:\n",
      "---\n",
      "- Document Named Entities (NER) - After Manual Update:\n",
      "Goku PERSON People, including fictional\n"
     ]
    }
   ],
   "source": [
    "# Updating Document Entities\n",
    "print('- Document Named Entities (NER) - Before Manual Update:')\n",
    "\n",
    "for entity in document.ents: print(entity.text, entity.label_, spacy.explain(entity.label_))\n",
    "\n",
    "###\n",
    "\n",
    "document.ents = [span_with_label]\n",
    "\n",
    "print('---\\n- Document Named Entities (NER) - After Manual Update:')\n",
    "\n",
    "for entity in document.ents: print(entity.text, entity.label_, spacy.explain(entity.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652e21f2-5c43-49a4-bd7a-8e8765e00a81",
   "metadata": {},
   "source": [
    "<h1 id='2-part-of-speech-pos-morphemes-and-sentence-segmentation' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>‚ú® | Part-of-Speech (POS), Morphemes and Sentence Segmentation</h1>\n",
    "\n",
    "When a document is created, the text is processed by the following Pipelines in order:\n",
    "\n",
    "- **tokenizer** - `transforms each word in a Token`;\n",
    "\n",
    "- **tok2vec** - `calculates the WordVector for the whole Document and for each Token. Word2Vec is the default algorithm used for this task in Spacy`;\n",
    "\n",
    "- **tagger** - `responsible to assign Tag and Part-of-Speech (POS) on each Token, that is, the grammatical role`;\n",
    "\n",
    "- **parser** - `responsible to assign the relationships of the Tokens in the text, such as Dependency Label and Syntatic Head`;\n",
    "\n",
    "- **lemmatizer** - `responsible to assign the Lemma (dictionary/base form) to Tokens`;\n",
    "\n",
    "- **attribute_ruler** - `responsible to process Tokens and assign information on them following specific rules and logic given by us. This Pipeline is normally used when Spacy cannot process well a certain word, phrase or a target language`;\n",
    "\n",
    "- **ner (Named Entity Recognition)** - `responsible to identify and assign Named Entities and their Labels`;\n",
    "\n",
    "- **textcat** - `responsible to assign categories to Documents following rules and logic given by us. This Pipeline is normally used on Text Classification projects. For instance, the rating 'Steins;Gate is an amazing show' should be classified as 'positive'`;\n",
    "\n",
    "- **merge_noun_chunks** - `responsible to merge multiple Tokens that represent a single noun. For instance, instead of 'Son Goku' be considered as two tokens, one for each word, it's considered as a single Token`;\n",
    "\n",
    "- **merge_entities** - `responsible to merge multiple Tokens that represent an entity. For example, instead of 'the Kame House' be considered as three Tokens, one for each word, it's considered as a single Token`.\n",
    "\n",
    "About the `tagger` Pipeline, Spacy provides two `Part-of-Speech (POS)`  Tags, the `coarse` that is accessible on `.pos_` attribute and the `fine-grained` that is accessbile on `._tag` attribute, being:\n",
    "\n",
    "- **coarse (.pos_)** - `based on Universal Dependencies Tag Set (https://universaldependencies.org/u/pos/all.html)`;\n",
    "\n",
    "- **fine-grained (.tag_)** - `based on OntoNotes 5.0, the dataset used to train the Language Model in Spacy (https://catalog.ldc.upenn.edu/LDC2013T19)`.\n",
    "\n",
    "By the way, `fine-grained (.tag_)` Part-of-Speech (POS) contains more information about the Token, for example, when a Token is a verb, it returns the aspect (1¬™, 2¬™, 3¬™ person; singular or plural) and the tense (past, present, future, conditional...); whereas `coarse (.pos_)` only returns that the Token is a verb and we must access the `Morphemes` attributes in order to get further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d999b960-fa78-426e-8f87-34e9c75ab08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Text: Hey\n",
      "- Coarse POS: INTJ - interjection\n",
      "- Fine-Grained POS: UH - interjection\n",
      "---\n",
      "- Text: it\n",
      "- Coarse POS: PRON - pronoun\n",
      "- Fine-Grained POS: PRP - pronoun, personal\n",
      "---\n",
      "- Text: 's\n",
      "- Coarse POS: AUX - auxiliary\n",
      "- Fine-Grained POS: VBZ - verb, 3rd person singular present\n",
      "---\n",
      "- Text: me\n",
      "- Coarse POS: PRON - pronoun\n",
      "- Fine-Grained POS: PRP - pronoun, personal\n",
      "---\n",
      "- Text: ,\n",
      "- Coarse POS: PUNCT - punctuation\n",
      "- Fine-Grained POS: , - punctuation mark, comma\n",
      "---\n",
      "- Text: Goku\n",
      "- Coarse POS: PROPN - proper noun\n",
      "- Fine-Grained POS: NNP - noun, proper singular\n",
      "---\n",
      "- Text: !\n",
      "- Coarse POS: PUNCT - punctuation\n",
      "- Fine-Grained POS: . - punctuation mark, sentence closer\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Coarse and Fine-Grained Part-of-Speech (POS)\n",
    "document = nlp_large('Hey it\\'s me, Goku!')\n",
    "\n",
    "for token in document:\n",
    "    print(f'- Text: {token.text}')\n",
    "    print(f'- Coarse POS: {token.pos_} - {spacy.explain(token.pos_)}')\n",
    "    print(f'- Fine-Grained POS: {token.tag_} - {spacy.explain(token.tag_)}')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f100be3e-b994-4111-808b-0f88b2a9aa59",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "`Morphemes` are the smalles piece of the words and, in Spacy, can be accessed through `.morph` attribute. Besides, not all Tokens contain morphemes attributes and when so, the available ones may differ from each Token.\n",
    "\n",
    "In order to get all available `Morphemes Attributes`, we can convert `.morph` into a dictionary; and to access the information, we can simply use `get` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13b5d564-96fc-4c0b-a4fb-c7ef6e92101d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Text: Hey\n",
      "- Morpheme Attributes: \n",
      "- Morpheme Attributes Dictionary: {}\n",
      "- Morpheme - Number: \n",
      "- Morpheme - Person: \n",
      "---\n",
      "- Text: it\n",
      "- Morpheme Attributes: Case=Nom|Gender=Neut|Number=Sing|Person=3|PronType=Prs\n",
      "- Morpheme Attributes Dictionary: {'Case': 'Nom', 'Gender': 'Neut', 'Number': 'Sing', 'Person': '3', 'PronType': 'Prs'}\n",
      "- Morpheme - Number: Sing\n",
      "- Morpheme - Person: 3\n",
      "---\n",
      "- Text: 's\n",
      "- Morpheme Attributes: Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
      "- Morpheme Attributes Dictionary: {'Mood': 'Ind', 'Number': 'Sing', 'Person': '3', 'Tense': 'Pres', 'VerbForm': 'Fin'}\n",
      "- Morpheme - Number: Sing\n",
      "- Morpheme - Person: 3\n",
      "---\n",
      "- Text: me\n",
      "- Morpheme Attributes: Case=Acc|Number=Sing|Person=1|PronType=Prs\n",
      "- Morpheme Attributes Dictionary: {'Case': 'Acc', 'Number': 'Sing', 'Person': '1', 'PronType': 'Prs'}\n",
      "- Morpheme - Number: Sing\n",
      "- Morpheme - Person: 1\n",
      "---\n",
      "- Text: ,\n",
      "- Morpheme Attributes: PunctType=Comm\n",
      "- Morpheme Attributes Dictionary: {'PunctType': 'Comm'}\n",
      "- Morpheme - Number: \n",
      "- Morpheme - Person: \n",
      "---\n",
      "- Text: Goku\n",
      "- Morpheme Attributes: Number=Sing\n",
      "- Morpheme Attributes Dictionary: {'Number': 'Sing'}\n",
      "- Morpheme - Number: Sing\n",
      "- Morpheme - Person: \n",
      "---\n",
      "- Text: !\n",
      "- Morpheme Attributes: PunctType=Peri\n",
      "- Morpheme Attributes Dictionary: {'PunctType': 'Peri'}\n",
      "- Morpheme - Number: \n",
      "- Morpheme - Person: \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Morphemes\n",
    "for token in document:\n",
    "    morpheme_dictionary = token.morph.to_dict()\n",
    "    morpheme_number = morpheme_dictionary.get('Number', '')\n",
    "    morpheme_person = morpheme_dictionary.get('Person', '')\n",
    "\n",
    "    print(f'- Text: {token.text}')\n",
    "    print(f'- Morpheme Attributes: {token.morph}')\n",
    "    print(f'- Morpheme Attributes Dictionary: {morpheme_dictionary}')\n",
    "    print(f'- Morpheme - Number: {morpheme_number}') # or token.morph.get('Number')\n",
    "    print(f'- Morpheme - Person: {morpheme_person}') # or token.morph.get('Person')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f1f297-521b-40be-9188-ecae2c99d21f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "`Sentence Segmentation` is very useful when dealing with large Documents. It's responsible to split the Document into sentences and we can access it through `.sents` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "406d3422-6ddd-4e6b-9c95-461abc0ba39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Sentence 0: Hey it's me, Goku!\n",
      "- Sentence 1: You look strong, let's fight!\n",
      "- Sentence 2: Oh, but let's head to the Kame House first.\n"
     ]
    }
   ],
   "source": [
    "# Sentence Segmentation\n",
    "document = nlp_large('Hey it\\'s me, Goku! You look strong, let\\'s fight! Oh, but let\\'s head to the Kame House first.')\n",
    "sentences = list(document.sents)\n",
    "for index, sentence in enumerate(sentences): print(f'- Sentence {index}: {sentence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be96a502-6239-4d49-8092-1cd069b25941",
   "metadata": {},
   "source": [
    "<h1 id='3-word-vectors-and-semantic-similarity' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>ü™û | Word Vectors and Semantic Similarity</h1>\n",
    "\n",
    "`Semantic Similarity` is a technique to check out how similar Documents, Spans and Tokens are between each other given their content and not context. The similarity goes from 0 (`non-similar`) to 1 (`totally similar`) and it's calculated via `Cosine Similarity` by default in Spacy.\n",
    "\n",
    "In order to be able to calculate the similarity, the NLP object should have `WordVectors`, that are calculated via `Word2Vec` algorithm by default in Spacy.\n",
    "\n",
    "However, only medium (md) and large (lg) pre-trained Pipelines contain WordVectors automatically calculated when creating Documents, Spans and Tokens. So, when working with small (sm), accuracy (trf) or blank models, we should calculate the WordVectors by ourselves:\n",
    "\n",
    "- **‚ùå blank** - `doesn't contain WordVectors`;\n",
    "- **‚ùå en_core_web_sm** - `doesn't contain WordVectors`;\n",
    "- **‚ùå en_core_web_trf** - `doesn't contain WordVectors`;\n",
    "- **‚úîÔ∏è en_core_web_md** - `contains WordVectors`;\n",
    "- **‚úîÔ∏è en_core_web_lg** - `contains WordVectors`.\n",
    "\n",
    "Besides, short phrases are bettern than long Documents and Spans with many irrelevant words (Stop Words?!) when calculating Semantic Similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3127290d-bc2d-49e7-8a03-4c989703a38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- WordVectors Size: 300\n",
      "- WordVectors: [-1.79141298e-01  2.59370029e-01 -6.19667135e-02 -9.03841406e-02\n",
      "  4.12608907e-02  4.53091338e-02  1.05849013e-01 -2.85014302e-01\n",
      " -1.73039287e-01  1.65766442e+00 -1.70944706e-01 -1.68457717e-01\n",
      "  3.90684120e-02  1.74515545e-02 -2.41932869e-01 -1.07065730e-01\n",
      "  1.13352863e-02  8.43292892e-01 -1.54243857e-01 -1.04621135e-01\n",
      "  1.19625010e-01 -4.73900791e-03  3.30229998e-02 -1.82213709e-01\n",
      " -2.81651430e-02  2.05937158e-02  2.95375735e-02 -5.45188524e-02\n",
      "  2.76406139e-01 -2.49356136e-01  5.31405434e-02  2.57778853e-01\n",
      " -3.96807445e-03  5.42552806e-02  4.83615696e-02 -5.21360002e-02\n",
      " -6.91885203e-02  1.26119420e-01 -1.37276560e-01 -8.89018178e-02\n",
      " -1.22366585e-01 -1.40172720e-01  1.59969963e-02 -1.77072324e-02\n",
      "  8.81782845e-02  1.93355709e-01 -2.18852863e-01 -1.02922000e-01\n",
      "  1.13547578e-01 -2.76979953e-02  5.00818565e-02  1.70085698e-01\n",
      "  4.49685715e-02  2.94317063e-02  2.10785540e-03  1.87033281e-01\n",
      " -4.05484326e-02  3.63524295e-02  6.01218566e-02 -1.11168861e-01\n",
      " -1.68361276e-01  3.66530083e-02 -1.03078291e-01 -4.38575225e-04\n",
      "  7.06882849e-02 -2.15182289e-01 -8.51119906e-02  1.39713138e-01\n",
      "  5.46318712e-03 -2.93385773e-03  4.45262976e-02  7.72042722e-02\n",
      "  1.98276848e-01  1.21724298e-02 -1.47765707e-02  2.26552263e-01\n",
      "  2.15510413e-01 -1.31898001e-01  4.77646478e-02  4.57864285e-01\n",
      " -8.61801505e-02  2.06205860e-01 -1.08452858e-02  1.71181574e-01\n",
      "  1.10417143e-01 -2.52310544e-01  4.95979995e-01 -5.85011423e-01\n",
      "  1.89050287e-01 -6.31843274e-03 -2.77284253e-02  1.25074714e-01\n",
      " -4.90882881e-02  3.97933014e-02  1.46486148e-01  5.14301844e-02\n",
      " -7.42337108e-02 -2.21818581e-01 -5.05502895e-02  1.17392845e-01\n",
      "  1.16177149e-01  6.88561201e-02 -2.18509302e-01  4.18028645e-02\n",
      "  3.91394310e-02 -4.89357039e-02  1.35714725e-01  8.00078586e-02\n",
      " -5.82657158e-02  3.49600008e-03  1.32781863e-01 -3.16934288e-01\n",
      "  1.06352912e-02 -2.24589840e-01  9.36246440e-02  1.98532864e-01\n",
      "  7.17960000e-02 -1.34200007e-01 -2.29568537e-02 -8.64222795e-02\n",
      "  7.04607069e-02 -2.83751376e-02  1.76406339e-01  1.67075291e-01\n",
      "  1.38839707e-01  1.69080570e-01 -1.01619229e-01 -1.58101723e-01\n",
      " -1.85031574e-02  5.34235761e-02 -1.50437862e-01 -2.47178882e-01\n",
      "  5.23982868e-02 -8.17412809e-02 -1.96999739e-04 -6.53428910e-03\n",
      " -1.04755713e-02  9.39071327e-02 -2.33906135e-01  9.31794271e-02\n",
      " -1.91726017e+00  2.12119982e-01  1.85177878e-01  4.33105715e-02\n",
      " -2.97407471e-02  1.25972584e-01 -2.82342881e-02  7.89200068e-02\n",
      " -3.11825126e-01 -6.32861406e-02 -6.56586885e-02  2.39967138e-01\n",
      "  2.58741111e-01  4.05271351e-03  1.24204941e-01 -1.21462427e-01\n",
      " -6.54208586e-02 -2.30658561e-01  3.59528586e-02  8.14985577e-03\n",
      " -7.52362981e-02  2.61546280e-02 -5.50669990e-02 -5.87625727e-02\n",
      " -1.01805709e-01 -1.46923706e-01  5.46652898e-02  3.30276862e-02\n",
      "  3.57650034e-02  1.26647577e-01 -1.80402547e-01 -1.00148447e-01\n",
      " -1.12642422e-01 -2.37858564e-01 -3.00831765e-01 -9.35582742e-02\n",
      " -5.06155789e-02 -5.28718752e-04 -1.91272467e-01 -1.87751371e-02\n",
      "  2.83602867e-02 -6.43054172e-02 -2.21388601e-02 -1.25668034e-01\n",
      "  1.20473281e-01 -1.40244991e-01 -5.26505709e-02 -6.77962825e-02\n",
      "  3.20702828e-02  1.35754263e-02  1.03920154e-01  2.39209980e-02\n",
      " -3.06512862e-01  4.31385636e-03 -2.94943224e-03 -1.80728603e-02\n",
      "  4.26117610e-03 -2.63903439e-01  1.58030584e-01  2.25987151e-01\n",
      " -1.03052855e-01  4.19755690e-02 -1.81682836e-02  2.39594262e-02\n",
      "  2.55090296e-01 -5.95849976e-02  7.25225732e-02 -8.34091455e-02\n",
      "  1.60303995e-01 -1.67881280e-01 -1.61628738e-01 -3.01075995e-01\n",
      " -4.10240404e-02  2.22435780e-02  1.93902031e-01  9.60151181e-02\n",
      "  1.15111865e-01 -2.39992235e-02 -1.27015978e-01  3.00235767e-02\n",
      " -4.63084280e-02  1.33515984e-01 -8.58137086e-02  3.71088907e-02\n",
      "  9.86999944e-02  9.08111408e-02 -1.17564440e-01  1.98359445e-01\n",
      "  8.59495178e-02 -7.46691376e-02 -2.79105276e-01 -2.15018559e-02\n",
      " -5.27844243e-02  1.25190154e-01 -1.49037138e-01  2.19715998e-01\n",
      " -4.47701477e-02 -1.59492746e-01 -5.88330030e-02 -1.14471465e-02\n",
      "  4.59578559e-02  1.24490976e-01 -5.24220727e-02  1.85525700e-01\n",
      "  1.70211151e-01 -1.60777435e-01  7.98248500e-03  1.98272876e-02\n",
      "  3.84387113e-02  2.95765430e-01  1.20030716e-01 -1.72947928e-01\n",
      " -1.17415003e-01 -2.05514464e-03  4.42055725e-02  2.05187157e-01\n",
      " -8.65781456e-02 -6.16351366e-02 -3.35857272e-04  4.05369997e-02\n",
      "  1.62322838e-02  1.12200722e-01  2.26114481e-03  1.06451288e-01\n",
      " -3.60468626e-02 -1.14656150e-01 -1.00898612e-02  8.72449949e-02\n",
      "  1.37767419e-01  1.79514438e-01  5.16042970e-02 -1.07146427e-01\n",
      " -1.67627245e-01 -4.51101400e-02 -1.71961293e-01  5.66019937e-02\n",
      " -1.55454293e-01 -1.56329989e-01  1.14061736e-01  2.63791263e-01\n",
      "  6.16871454e-02 -2.50881445e-02  1.91590302e-02 -1.00175686e-01\n",
      " -4.47324291e-02 -1.81271345e-03  2.21022144e-01 -1.31819278e-01\n",
      "  5.42354286e-02 -2.14773133e-01  4.75871377e-02 -3.44121456e-02\n",
      " -5.95957180e-03 -1.89412870e-02  2.23537296e-01 -1.55403420e-01\n",
      " -4.14735712e-02 -6.20954297e-02  4.00615335e-02  1.33471236e-01]\n"
     ]
    }
   ],
   "source": [
    "# Accessing WordVector object of a Document via '.vector' list\n",
    "document0 = nlp_large('Hey it\\'s me, Goku!')\n",
    "print(f'- WordVectors Size: {len(document0.vector)}')\n",
    "print(f'- WordVectors: {document0.vector}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0d93d60-d281-44c0-9dee-399b6816c4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- WordVectors Size: 300\n",
      "- WordVectors: [-4.0001e-01 -6.4200e-01  5.4013e-01 -4.6932e-01  2.3678e-01 -2.1087e-01\n",
      "  6.1721e-01  1.7844e-01 -3.0992e-02 -8.0075e-01 -2.2622e-02 -5.7395e-01\n",
      " -2.0335e-01  5.6272e-01 -2.1141e-01 -1.8668e-01  4.7549e-01  2.4373e-01\n",
      " -3.3346e-01 -2.6711e-01  4.7203e-01 -2.5117e-01  2.1239e-01 -9.1873e-01\n",
      " -1.7530e-01  3.1297e-01 -3.4612e-02 -3.0685e-01 -6.7977e-02 -3.3142e-01\n",
      "  8.2209e-02  6.5252e-02  5.3315e-01 -4.8743e-02  1.8212e-01 -2.1009e-01\n",
      " -9.8826e-01 -1.4896e-01 -5.1341e-01 -2.7604e-02 -4.3536e-01 -9.4728e-01\n",
      "  2.3615e-01  4.6428e-01 -2.3510e-01  1.9391e-01 -1.4218e-01 -4.6251e-01\n",
      " -1.9442e-01 -4.2031e-01  5.2424e-02 -3.2733e-01  5.8309e-01 -5.6361e-01\n",
      " -2.2144e-01  1.0501e+00 -1.4222e-02  3.3922e-01  1.4471e-01 -4.2506e-01\n",
      "  3.8405e-02  5.8688e-01 -1.6905e-01 -4.0435e-01 -2.8506e-01 -7.6365e-02\n",
      "  1.0794e-01  4.7223e-01  4.4349e-02 -4.1628e-01 -4.7326e-01 -4.2144e-02\n",
      "  1.1894e-01  1.2493e-02  3.9658e-02  7.8624e-01  4.5608e-01 -2.4868e-01\n",
      "  6.6457e-01  6.0021e-01 -2.2850e-01  5.3138e-01  7.4252e-02  3.8885e-01\n",
      "  1.2449e-01  1.2114e-02  1.2864e+00 -7.9519e-01 -7.1542e-01 -5.6401e-01\n",
      " -1.7614e-01  2.9771e-01 -5.7507e-01 -5.2980e-02  2.3526e-01 -1.4328e-02\n",
      "  3.0203e-02 -4.5622e-01  5.4788e-01  2.1158e-01  6.6112e-01  6.7305e-01\n",
      " -3.4382e-01  2.8099e-01  1.3968e-01 -9.8726e-01 -1.7077e-01  3.6486e-01\n",
      "  1.7935e-01  2.3934e-02  1.4860e-01 -8.0594e-01 -1.1645e+00 -9.9446e-01\n",
      "  6.4812e-01  1.0880e+00  1.4618e-01  5.6402e-01 -1.4049e-01 -8.6346e-02\n",
      " -7.1477e-01  3.7064e-01  3.8566e-01  6.3988e-01  6.0357e-01  5.5763e-01\n",
      " -7.3151e-01 -4.6396e-01 -2.4343e-01  4.1922e-01 -8.6735e-01  6.9078e-02\n",
      "  2.0558e-01 -3.5848e-01 -6.2264e-02 -5.3331e-02  1.8893e-01  3.3667e-02\n",
      " -4.2068e-01  4.0396e-01 -1.9623e+00 -4.9793e-01 -4.0744e-01  1.2224e-01\n",
      "  8.2833e-03  7.8070e-01  3.4247e-01 -5.8833e-01 -5.6036e-01 -1.9274e-01\n",
      " -1.2193e-01  1.7476e-01  7.9979e-01 -1.2130e-01  2.4060e-01 -2.7522e-01\n",
      " -3.1650e-01 -3.2148e-01  3.3525e-01  4.8111e-01  2.5161e-01 -1.5320e-01\n",
      " -2.5478e-01  3.7141e-02  2.6646e-01 -6.5442e-01 -1.7368e-01  3.8886e-01\n",
      " -6.1952e-01 -3.9240e-02 -7.0462e-01 -1.4632e-03 -1.9323e-01 -1.5834e-01\n",
      " -6.8510e-01 -5.0341e-01 -4.8850e-01  5.7003e-01 -3.5946e-01  6.2104e-02\n",
      "  2.0115e-01  2.1873e-01  4.1476e-01 -3.8281e-01  1.9678e-01 -3.9696e-02\n",
      " -1.0645e-01 -2.6035e-01  1.1005e-01  2.5890e-01 -1.8186e-02  6.0195e-01\n",
      " -1.1712e+00  3.6529e-01  2.4761e-01 -5.7059e-01 -1.9877e-04 -5.0384e-02\n",
      "  1.5967e-01  1.5423e-01  6.9353e-01 -1.2648e-01  6.7681e-01  5.5164e-01\n",
      "  6.7908e-01 -1.2545e-01 -6.4718e-02 -5.9634e-01  2.2663e-01 -1.7617e-01\n",
      "  1.2049e-03  1.5678e-02 -2.0383e-01  8.9327e-01  1.6602e-01 -6.0776e-01\n",
      "  5.2095e-01  3.6794e-01  6.7023e-01 -1.5680e-01  2.7839e-01 -5.0026e-01\n",
      "  9.1281e-02 -2.8618e-03 -2.4985e-01  1.1022e-01 -2.3338e-01  6.2311e-02\n",
      "  7.5049e-01 -1.3958e-02 -5.5921e-01 -6.1496e-01 -6.7822e-02  1.0072e-01\n",
      " -6.1184e-01  1.3712e+00  3.7505e-02 -2.0750e-01 -1.6233e-01 -5.4304e-01\n",
      " -2.7253e-01  2.1501e-02 -3.8133e-01  5.0279e-01  1.3086e-01 -4.5309e-01\n",
      " -5.2407e-02  3.5977e-01  5.2243e-01  7.1908e-02  1.2441e-01 -8.7933e-01\n",
      " -5.4115e-02  2.1343e-01  5.2629e-01  9.7995e-02 -3.5835e-01  4.2639e-02\n",
      " -2.6502e-02 -2.0983e-02  1.6814e-01  1.7620e-01  1.7536e-01  1.9299e-01\n",
      "  5.8563e-02 -1.5994e-01 -6.8927e-01  6.7921e-01 -8.4516e-01  4.9028e-01\n",
      "  2.8299e-01 -3.4533e-01  1.4554e-01  2.5961e-01 -1.5540e-01  1.4720e-01\n",
      " -9.9668e-01 -4.3692e-01  4.0953e-01  3.3818e-01 -6.1039e-01  4.6887e-02\n",
      "  8.8048e-03 -8.7238e-03  4.3988e-01  1.6086e-01  3.7858e-01 -5.3681e-01\n",
      " -1.3553e-01 -7.3839e-01  9.6171e-01  1.4430e-01 -1.7855e-01 -1.6132e-01\n",
      "  3.3277e-01  4.3376e-02 -3.0405e-01 -1.2109e-01  5.4114e-01  7.4098e-01]\n"
     ]
    }
   ],
   "source": [
    "# Accessing WordVector object of a Token via '.vector' list\n",
    "token0 = document0[5]\n",
    "print(f'- WordVectors Size: {len(token0.vector)}')\n",
    "print(f'- WordVectors: {token0.vector}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a12dca-cf0b-4cde-a813-bc63d5892e59",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e63d09e3-32fc-478c-96c4-f0b9ef11c845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Similarity between Documents: 0.9358318464113806\n"
     ]
    }
   ],
   "source": [
    "# Similarity between Documents\n",
    "document1 = nlp_large('I love pizza')\n",
    "document2 = nlp_large('I love pasta')\n",
    "print(f'- Similarity between Documents: {document1.similarity(document2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d0b74a7-001d-42fc-b83e-7d537c710f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Similarity between Tokens: 0.7369545698165894\n"
     ]
    }
   ],
   "source": [
    "# Similarity between Tokens\n",
    "document3 = nlp_large('I love pizza and pasta')\n",
    "token1 = document3[2]\n",
    "token2 = document3[4]\n",
    "print(f'- Similarity between Tokens: {token1.similarity(token2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1da32548-5a13-4675-925b-d7a7d95511f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Similarity between Document and Token: 0.5415431108130979\n"
     ]
    }
   ],
   "source": [
    "# Similarity between Document and Token\n",
    "document4 = nlp_large('I love pizza')\n",
    "token = nlp_large('cheese')[0]\n",
    "print(f'- Similarity between Document and Token: {document4.similarity(token)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14afe6e1-e75d-4c3f-9505-fad2af122f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Similarity between Span and Document: 0.5886225771237401\n",
      "- Similarity between Document and Span: 0.5886225771237401\n"
     ]
    }
   ],
   "source": [
    "# Similarity between Span and Document\n",
    "document5 = nlp_large('McDonalds sells burger')\n",
    "span = nlp_large('I like pizza and pasta')[2:5]\n",
    "print(f'- Similarity between Span and Document: {span.similarity(document5)}')\n",
    "print(f'- Similarity between Document and Span: {document5.similarity(span)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc735a2-3257-431d-9c38-4cb1c5d4a986",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Besides, similarity doesn't recognize `sentiments`. For instance, the two phrases `I love cats` and `I hate cats` have high similarity even though their meanings are totally opposite.\n",
    "\n",
    "It happens due to their semantic contents be very similar: both contains the words `I` followed by a `VERB` and then the word `cats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64fbd2e6-bf94-4779-b0ec-8e58836a8c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Similarity between Documents: 0.9409261755229907\n"
     ]
    }
   ],
   "source": [
    "# Semantic Similarity doesn't consider Sentiments but only Contents\n",
    "document6 = nlp_large('I love cats')\n",
    "document7 = nlp_large('I hate cats')\n",
    "print(f'- Similarity between Documents: {document6.similarity(document7)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cce4e9-5615-46b2-a4f3-c8c02e6e55a1",
   "metadata": {},
   "source": [
    "<h1 id='4-combining-predictions-and-rules' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>üé® | Combining Predictions and Rules</h1>\n",
    "\n",
    "The combination of Predictions from `Statistical Models` and Rules from `Rule-Based Systems` is a powerful technique to boost searches and Document processings.\n",
    "\n",
    "They are, literally, the combination of `context-dependent` and `context-independent` (both predictions) and texts (rules) during searches and matches over the Documents. So:\n",
    "\n",
    "- **Statistical Models** - `searches for generalized info, such as Named Entities (NER), Part-of-Speech (POS), Dependency Label and Syntatic Head of Tokens`;\n",
    "- **Rule-Based Systems** - `searches for specific, finite info, such as Specific Named Entities (countries of the world, soccer player names and dog breeds). We can achieve it by using Tokenizer, Matcher and PhraseMatcher objects from Spacy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a332b8ba-114a-4ffa-9662-bc495acb0fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Text: Golden Retriever\n",
      "- Root: Retriever\n",
      "- Part-of-Speech (POS): PROPN (proper noun)\n",
      "- Dependency Label: dobj (direct object)\n",
      "- Syntatic Head: have\n",
      "- Previous Token: a\n"
     ]
    }
   ],
   "source": [
    "# Combining Predictions from Statistical Modes\n",
    "# and Rules from Rule-Based Systems\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "document = nlp_large('I have a Golden Retriever')\n",
    "\n",
    "pattern = [{ 'LOWER': 'golden' }, { 'LOWER': 'retriever' }]\n",
    "\n",
    "matcher = Matcher(nlp_large.vocab)\n",
    "matcher.add('DOG_BREED', [pattern])\n",
    "matches = matcher(document)\n",
    "\n",
    "for match_id, start_index, end_index in matches:\n",
    "    matched_span = document[start_index:end_index]\n",
    "    print(f'- Text: {matched_span.text}')\n",
    "    print(f'- Root: {matched_span.root.text}') # Token that decides the Category of the Span\n",
    "    print(f'- Part-of-Speech (POS): {matched_span.root.pos_} ({spacy.explain(matched_span.root.pos_)})')\n",
    "    print(f'- Dependency Label: {matched_span.root.dep_} ({spacy.explain(matched_span.root.dep_)})')\n",
    "    print(f'- Syntatic Head: {matched_span.root.head.text}')\n",
    "    print(f'- Previous Token: {document[start_index-1].text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515534e6-ddc4-4620-a692-7279453cf760",
   "metadata": {},
   "source": [
    "<h1 id='5-phrasematcher-morphological-attributes-matcher-and-dependencymatcher' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>üîç | PhraseMatcher, Morphological Attributes Matcher and Dependency Matcher</h1>\n",
    "\n",
    "Differently to Matcher, `PhraseMatchers` are more efficient and faster when we desire to search for a specific list of strings and instead of receiving a dictionaries with search rules, it receives only the list of strings that we desire to search and match. Oh, and always remember to convert this list into a `nlp.pipe` in order to gain more efficiency and save memory.\n",
    "\n",
    "`Morphological Attributes Matcher` consists in simple Matchers that aims to filter using only morphological attributes rather than Part-of-Speech, Lemmas or other info.\n",
    "\n",
    "`Dependency Matcher`-wise, it's responsible to apply filters using Syntatic Patterns more efficiently, that is, Dependency Label and Syntatic Head filters.\n",
    "\n",
    "Then:\n",
    "\n",
    "- **Matcher and Morphological Attributes Matcher** - `useful when searching for Tokens or Spans with simple filters, such as, Morphological and Part-of-Speech (POS) attributes`;\n",
    "\n",
    "- **PhraseMatcher** - `useful when searching for specific Spans or Named Entities Recognition (NER)`;\n",
    "\n",
    "- **DependencyMatcher** - `useful when searching for Tokens and Spans taking the Dependency Label relationship into consideration`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1989f1f8-95f0-40e2-abee-5e67f1db8351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fetched Pattern: indMood+3Person\n",
      "- Matched Span: 's\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Morphological Attributes - Not as Spans\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "document = nlp_large('Hey it\\'s me, Goku!')\n",
    "pattern_1 = [{ 'MORPH': 'Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin' }] # instead of passing the whole morphological attribute\n",
    "pattern_2 = [{ 'MORPH': { 'IS_SUPERSET': ['Mood=Ind', 'Person=3'] } }] # we can get advantage of 'IS_SUPERSET' attribute!!\n",
    "\n",
    "morphological_matcher = Matcher(nlp_large.vocab)\n",
    "morphological_matcher.add('indMood+3Person', [pattern_2])\n",
    "matches = morphological_matcher(document)\n",
    "\n",
    "for match_id, start_index, end_index in matches:\n",
    "    pattern_name = nlp_large.vocab[match_id].text\n",
    "    matched_span = document[start_index:end_index]\n",
    "\n",
    "    print(f'- Fetched Pattern: {pattern_name}')\n",
    "    print(f'- Matched Span: {matched_span.text}')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ab11054-ced5-4fc4-9106-183aae79f0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Matched Span: 's\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Morphological Attributes - as Spans\n",
    "matches = morphological_matcher(document, as_spans=True)\n",
    "\n",
    "for matched_span in matches:\n",
    "    print(f'- Matched Span: {matched_span}')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b9af7e-6638-4963-9817-abd11956c4f1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "276ddc89-bc3c-471a-b207-42d50b3900c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Pattern Name: dog-breed\n",
      "- Matched Span: Golden Retriever\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# PhraseMatcher\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "document = nlp_large('I have a Golden Retriever')\n",
    "\n",
    "pattern = ['Golden Retriever']\n",
    "\n",
    "phraseMatcher = PhraseMatcher(nlp_large.vocab)\n",
    "phraseMatcher.add('dog-breed', nlp_large.pipe(pattern))\n",
    "matches = phraseMatcher(document)\n",
    "\n",
    "for match_id, start_index, end_index in matches:\n",
    "    pattern_name = nlp_large.vocab[match_id].text\n",
    "    matched_span = document[start_index:end_index]\n",
    "    \n",
    "    print(f'- Pattern Name: {pattern_name}')\n",
    "    print(f'- Matched Span: {matched_span.text}')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1237ae4-619a-4039-8e8d-e3cfd13a3095",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf7b6a3a-5fad-4ddf-8cec-98d22be7525b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Pattern Name: verb-nsubj+verb-dobj\n",
      "- Head Token: returned\n",
      "- Nominal Subject Token: protestors\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Dependency Matcher\n",
    "from spacy.matcher import DependencyMatcher\n",
    "\n",
    "document = nlp_large('On 17 September 2012, protestors returned to Zuccotti Park to mark the one-year anniversary of the beginning of the occupation.')\n",
    "\n",
    "pattern = [\n",
    "    { 'RIGHT_ID': 'verb', 'RIGHT_ATTRS': { 'POS': 'VERB' } } # head of the document\n",
    "    , { 'LEFT_ID': 'verb', 'REL_OP': '>', 'RIGHT_ID': 'subject', 'RIGHT_ATTRS': { 'DEP': 'nsubj' } } # nominal subject directly related to the head\n",
    "    #   , { 'LEFT_ID': 'verb', 'REL_OP': '>', 'RIGHT_ID': 'd_object', 'RIGHT_ATTRS': { 'DEP': 'dobj' } } # direct object directly related to the head\n",
    "]\n",
    "\n",
    "dependencyMatcher = DependencyMatcher(nlp_large.vocab)\n",
    "dependencyMatcher.add('verb-nsubj+verb-dobj', [pattern])\n",
    "matches = dependencyMatcher(document)\n",
    "\n",
    "for match_id, matched_token_ids in matches:\n",
    "    pattern_name = nlp_large.vocab[match_id].text\n",
    "    matched_token_1 = document[matched_token_ids[0]]\n",
    "    matched_token_2 = document[matched_token_ids[1]]\n",
    "\n",
    "    print(f'- Pattern Name: {pattern_name}')\n",
    "    print(f'- Head Token: {matched_token_1.text}')\n",
    "    print(f'- Nominal Subject Token: {matched_token_2.text}')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c6c3f4-5229-4635-b9b1-838dad47f09b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h1 id='reach-me' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>üì´ | Reach Me</h1>\n",
    "\n",
    "> **Email** - [csfelix08@gmail.com](mailto:csfelix08@gmail.com?)\n",
    "\n",
    "> **Linkedin** - [linkedin.com/in/csfelix/](https://www.linkedin.com/in/csfelix/)\n",
    "\n",
    "> **GitHub:** - [CSFelix](https://github.com/CSFelix)\n",
    "\n",
    "> **Kaggle** - [DSFelix](https://www.kaggle.com/dsfelix)\n",
    "\n",
    "> **Portfolio** - [CSFelix.io](https://csfelix.github.io/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "574937d0-4668-4411-b145-66b99d961e78",
   "metadata": {},
   "source": [
    "<center>    \n",
    "    <h1 id='spacy-notebook-6' style='color:#7159c1; font-size:350%'>Steps to Train a Language Model</h1>\n",
    "    <i style='font-size:125%'>Training a Neural Network Model</i>\n",
    "</center>\n",
    "\n",
    "> **Topics**\n",
    "\n",
    "```\n",
    "- üíª Applications\n",
    "- üè∑Ô∏è Training Named Entity Recognition (NER) Pipeline\n",
    "- üíæ Generating Binary Corpus\n",
    "- üìù Training Config File (Single Source of Truth)\n",
    "- üí™ Training Pipeline\n",
    "- ü™à Loading Pipeline\n",
    "- üì¶ Packaging Pipeline\n",
    "- ü•á Best Practices for Training Neural Network Models with Spacy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a12836-8c8c-4d4c-9e58-0b03f0a70594",
   "metadata": {},
   "source": [
    "<h1 id='0-applications' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>üíª | Applications</h1>\n",
    "\n",
    "Spacy allows us to train and update our own model with our specific dataset, turning possible more advanced NLP tasks, such as `Text Classification`, `Specific Named Entity Recognition (NER)`, `Improvements in Tagger (Tag and Part-of-Speech [POS]) and Parser (Dependency Label and Syntatic Head) Pipelines`.\n",
    "\n",
    "Normally, every model goes through the following six steps (list and Figure 1) over the training phase:\n",
    "\n",
    "1. Initialize the model with random weights;\n",
    "2. Predict a few examples with the current weights;\n",
    "3. Compare the predicted results with the real labels;\n",
    "4. Calculate the changes to improve the weights;\n",
    "5. Slightly update and improve the weights;\n",
    "6. Go back to step 2.\n",
    "\n",
    "<figure style='text-aling:center'>\n",
    "    <img style='border-radius:20px' src='./images/3-steps-to-update-our-own-model.png' alt='Diagram of Training Steps of Neural Network Models in Spacy' />\n",
    "    <figcaption>Figure 1 - Diagram of Training Steps of Neural Network Models in Spacy. By <a href='https://course.spacy.io/en/chapter4'>Spacy - Advanced NLP with Spacy Course - Chapter 4</a>.</figcaption>\n",
    "</figure>\n",
    "\n",
    "The cycle is repeated until good values for weights are achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9936850-d5a8-4e36-900d-9db575b636d6",
   "metadata": {},
   "source": [
    "<h1 id='1-training-named-entity-recognition-ner-pipeline' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>üè∑Ô∏è | Training Named Entity Recognition (NER) Pipeline</h1>\n",
    "\n",
    "In this notebook, let's focus on training the Named Entity Recognition (NER) Pipeline, making the model learn a new group of entity, as well as, new words.\n",
    "\n",
    "The sample size for the training phase can vary accordingly to our goals and actions, for instance:\n",
    "\n",
    "- **Update an Existing Model** - `a few hundred to a few thousand examples`;\n",
    "- **Train a New Category** - `a few thousand to a million examples`;\n",
    "- **Spacy's English Model** - `2 million words and examples`.\n",
    "\n",
    "For now, let's train a model to be able to identify `iPhone X` as a `GADGET` entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a40222c-00aa-4080-9e54-914a9f2dcad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Dataset\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp_en_blank = spacy.blank('en')\n",
    "\n",
    "document1 = nlp_en_blank('iPhone X is coming')\n",
    "document1.ents = [Span(document1, 0, 2, label='GADGET')]\n",
    "\n",
    "document2 = nlp_en_blank('I need a new phone. Any tips?')\n",
    "document2.ents = []\n",
    "\n",
    "documents = [document1, document2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "037c653c-6a01-4427-bd82-d1fc79d194d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Data into Train (50.00%) and Validation (50.00%)\n",
    "import random\n",
    "\n",
    "random.shuffle(documents)\n",
    "threshold = len(documents) // 2\n",
    "train_documents = documents[:threshold]\n",
    "valid_documents = documents[threshold:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a81d24-cbd6-47f2-b509-ec405a6d9ac9",
   "metadata": {},
   "source": [
    "<h1 id='2-generating-binary-corpus' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>üíæ | Generating Binary Corpus</h1>\n",
    "\n",
    "Before training the model, we must to export both training and validation datasets into binary files known as `DocBin`.\n",
    "\n",
    "- **DocBin** - `file that serielizes and stores Documents in binary. Besides, since it only stores the shared vocabulary once, it's faster than Pickle`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e115c4c7-0ecf-4290-8f3c-de2d650ba76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Binary Corpus\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "train_binary_documents = DocBin(docs=train_documents)\n",
    "train_binary_documents.to_disk('./language-model/datasets/train.spacy')\n",
    "\n",
    "valid_binary_documents = DocBin(docs=valid_documents)\n",
    "valid_binary_documents.to_disk('./language-model/datasets/valid.spacy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cca39b-3f37-4ff0-927d-83fd8bd3feab",
   "metadata": {},
   "source": [
    "Binary Documents in Spacy normally uses `.spacy` extension for all generated files, however, sometimes we already have our training and validation data and we must convert it to Spacy's binary format. In order to do it, we must use the following command:\n",
    "\n",
    "```bash\n",
    "python -m spacy convert ./language-model/datasets/train.gold.conll ./datasets\n",
    "```\n",
    "\n",
    "The conversion accepts the following extensions to be converted into Spacy's binary format: `['.conll', '.conll', '.iob']`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307fc25a-c247-4fd2-ad8c-38a03771a779",
   "metadata": {},
   "source": [
    "<h1 id='3-training-config-file-single-source-of-truth' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>üìù | Training Config File (Single Source of Truth)</h1>\n",
    "\n",
    "The Config File, normally named as `config.cfg`, is a file where we can tell how the NLP object must be initialized, which Pipeline components must be added to the model, how the model's internal configurations should be configured, how to load the training and validation data, and the hyperparameter values.\n",
    "\n",
    "But don't worry, there is no need to write this whole file by hand, because we can use a Spacy's command to generate a default file and then update it accordingly to our needs. The command is the following:\n",
    "\n",
    "```bash\n",
    "python -m spacy init config ./language-model/configs/config.cfg --lang en --pipeline tagger,parser,ner,lemmatizer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ed9a5f1-c8b0-4819-8e4f-43e408d1e3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m[!] To generate a more effective transformer-based config (GPU-only),\n",
      "install the spacy-transformers package and re-run this command. The config\n",
      "generated now does not use transformers.\u001b[0m\n",
      "\u001b[38;5;4m[i] Generated config template specific for your use case\u001b[0m\n",
      "- Language: en\n",
      "- Pipeline: ner\n",
      "- Optimize for: efficiency\n",
      "- Hardware: CPU\n",
      "- Transformer: None\n",
      "\u001b[38;5;2m[+] Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m[+] Saved config\u001b[0m\n",
      "language-model\\configs\\config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init config ./language-model/configs/config.cfg --lang en --pipeline ner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ec7ffb-bb72-4e51-86ec-475d5c6c7123",
   "metadata": {},
   "source": [
    "<h1 id='4-training-pipeline' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>üí™ | Training Pipeline</h1>\n",
    "\n",
    "After creating the binary files of Training and Validation Documents and setting the Config file, we are ready to train our model using the following command:\n",
    "\n",
    "```bash\n",
    "python -m spacy train ./language-model/configs/config.cfg --output ./language-model/output --paths.train ./language-model/datasets/train.spacy --paths.dev ./language-model/datasets/valid.spacy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21f82922-d281-41d4-b1d8-2bf020dfeba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m[+] Created output directory: language-model\\output\u001b[0m\n",
      "\u001b[38;5;4m[i] Saving to output directory: language-model\\output\u001b[0m\n",
      "\u001b[38;5;4m[i] Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m[+] Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4m[i] Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4m[i] Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00      4.50    0.00    0.00    0.00    0.00\n",
      "200     200          0.01     14.26    0.00    0.00    0.00    0.00\n",
      "400     400          0.00      0.00    0.00    0.00    0.00    0.00\n",
      "600     600          0.00      0.00    0.00    0.00    0.00    0.00\n",
      "800     800          0.00      0.00    0.00    0.00    0.00    0.00\n",
      "1000    1000          0.00      0.00    0.00    0.00    0.00    0.00\n",
      "1200    1200          0.00      0.00    0.00    0.00    0.00    0.00\n",
      "1400    1400          0.00      0.00    0.00    0.00    0.00    0.00\n",
      "1600    1600          0.00      0.00    0.00    0.00    0.00    0.00\n",
      "\u001b[38;5;2m[+] Saved pipeline to output directory\u001b[0m\n",
      "language-model\\output\\model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train ./language-model/configs/config.cfg --output ./language-model/output --paths.train ./language-model/datasets/train.spacy --paths.dev ./language-model/datasets/valid.spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99302cfa-213c-490e-855a-94f9b1974217",
   "metadata": {},
   "source": [
    "<h1 id='5-loading-pipeline' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>ü™à | Loading Pipeline</h1>\n",
    "\n",
    "After training the model, the last and the best Pipelines are stored into disk and we can load them in the same way we do with `en_core_web_lg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6648533c-b231-4f9e-82aa-fc5fc3e8b487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Pipeline\n",
    "nlp_best_pipeline = spacy.load('./language-model/output/model-best')\n",
    "nlp_last_pipeline = spacy.load('./language-model/output/model-last')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6195b32b-2fb6-45dd-ba64-203c870d7928",
   "metadata": {},
   "source": [
    "<h1 id='6-packaging-pipeline' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>üì¶ | Packaging Pipeline</h1>\n",
    "\n",
    "In the end, we can package a Pipeline in order to it become a Python Package and turn it installable!! To do it, we first should run the following command:\n",
    "\n",
    "```bash\n",
    "python -m spacy package ./language-model/output/model-best ./packages --name my_pipeline --version 1.0.0\n",
    "```\n",
    "\n",
    "Install it into any project we will be working on:\n",
    "\n",
    "```bash\n",
    "pip install ./language-model/output/en_model_best-1.0.0\n",
    "```\n",
    "\n",
    "And then load the Pipeline int our project:\n",
    "\n",
    "```python\n",
    "nlp = spacy.load('en_my_pipeline')\n",
    "```\n",
    "\n",
    "Realize that Spacy automatically adds the language the Pipeline has been trained on at the beginning of the package's name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47c8159-9e4b-415f-ab3c-aba40607e2ea",
   "metadata": {},
   "source": [
    "<h1 id='7-best-practices-for-training-neural-network-models-with-spacy' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>ü•á | Best Practices for Training Neural Network Models with Spacy</h1>\n",
    "\n",
    "- **Catastrophic Forgetting Problem**\n",
    "\n",
    "Description: when we update the model with a bunch of examples with a especific label, such as 'CARS', the model can `'unlearn'` how to predict 'PERSON' labels.\n",
    "\n",
    "Solution: always mix in examples of what the model previously got correct, for example, when training the model with 'CARS' labels, also include examples of 'PERSON' labels.\n",
    "\n",
    "- **Models Can't Learn Everything**\n",
    "\n",
    "Description: models can only make predictions based on Local Context, that is, based on the Context present in the examples it has been trained on. So, the model may not recognize and learn all patterns as expected.\n",
    "\n",
    "Solution: always stick with general labels rather than too specific ones, thus always prefer working with 'CLOTHING' label rather than 'ADULT_CLOTHING' and 'CHILDREN_CLOTHING' labels. Besides, we can add a `Rule-Based System` (Matcher or PhraseMatcher) in order to go from generic label to specific labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcffd6f6-8f74-4914-b783-0c1a96017413",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Observations: when dealing with NER, consider using `Doccano` for small and medium projects, and `INCEpTION` for large projects. They will help us automate the Specific Named Entity (NER) Recognition process.\n",
    "\n",
    "- <a href='https://github.com/doccano/doccano'>Doccano</a>\n",
    "\n",
    "- <a href='https://github.com/inception-project/inception'>INCEpTION</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baed1f55-cf36-42be-a7d2-6b023ae214b8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h1 id='reach-me' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>üì´ | Reach Me</h1>\n",
    "\n",
    "> **Email** - [csfelix08@gmail.com](mailto:csfelix08@gmail.com?)\n",
    "\n",
    "> **Linkedin** - [linkedin.com/in/csfelix/](https://www.linkedin.com/in/csfelix/)\n",
    "\n",
    "> **GitHub:** - [CSFelix](https://github.com/CSFelix)\n",
    "\n",
    "> **Kaggle** - [DSFelix](https://www.kaggle.com/dsfelix)\n",
    "\n",
    "> **Portfolio** - [CSFelix.io](https://csfelix.github.io/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

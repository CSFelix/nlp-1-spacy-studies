{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35592998-a763-49ee-9571-fe29ee3ece3d",
   "metadata": {},
   "source": [
    "<center>    \n",
    "    <h1 id='spacy-notebook-3' style='color:#7159c1; font-size:350%'>Processing Pipelines</h1>\n",
    "    <i style='font-size:125%'>Exploring more about Pipelines</i>\n",
    "</center>\n",
    "\n",
    "> **Topics**\n",
    "\n",
    "```\n",
    "- ü™à Pipelines - Part II\n",
    "- ‚ùå Analysing Pipelines Problems\n",
    "- ü™à New Pipelines\n",
    "- üé® Custom Pipelines Components\n",
    "- üß© Extension Types: Attributes, Properties and Methods\n",
    "- üìà Scaling and Performance\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccc3e34-699d-4a53-a6b9-f0b67501c235",
   "metadata": {},
   "source": [
    "<h1 id='0-pipelines-part-ii' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>ü™à | Pipelines Part (II)</h1>\n",
    "\n",
    "When a text is converted into a Document in Spacy, it's automatically processed by `Pipelines` in order to extract information about it, such as lemma, Part-of-Speech, Dependency Label and Entities.\n",
    "\n",
    "Normally, pre-trained models in Spacy contain the following seven Pipelines:\n",
    "\n",
    "- **tokenizer** - `transforms each word in a Token`;\n",
    "\n",
    "- **tok2vec** - `calculates the WordVector for the whole Document and for each Token. Word2Vec is the default algorithm used for this task in Spacy`;\n",
    "\n",
    "- **tagger** - `responsible to assign Tag and Part-of-Speech (POS) on each Token, that is, the grammatical role`;\n",
    "\n",
    "- **parser** - `responsible to assign the relationships of the Tokens in the text, such as Dependency Label and Syntatic Head`;\n",
    "\n",
    "- **lemmatizer** - `responsible to assign the Lemma (dictionary/base form) to Tokens`;\n",
    "\n",
    "- **attribute_ruler** - `responsible to process Tokens and assign information on them following specific rules and logic given by us. This Pipeline is normally used when Spacy cannot process well a certain word, phrase or a target language`;\n",
    "\n",
    "- **ner (Named Entity Recognition)** - `responsible to identify and assign Named Entities and their Labels`;\n",
    "\n",
    "- **textcat** - `responsible to assign categories to Documents following rules and logic given by us. This Pipeline is normally used on Text Classification projects. For instance, the rating 'Steins;Gate is an amazing show' should be classified as 'positive'`;\n",
    "\n",
    "- **merge_noun_chunks** - `responsible to merge multiple Tokens that represent a single noun. For instance, instead of 'Son Goku' be considered as two tokens, one for each word, it's considered as a single Token`;\n",
    "\n",
    "- **merge_entities** - `responsible to merge multiple Tokens that represent an entity. For example, instead of 'the Kame House' be considered as three Tokens, one for each word, it's considered as a single Token`.\n",
    "\n",
    "The two images below illustrates the Pipelines architecture in Spacy:\n",
    "\n",
    "<figure style='text-aling:center'>\n",
    "    <img style='border-radius:20px' src='./images/2.0-text-processing.png' alt='Diagram of Pipelines Architecture in Spacy' />\n",
    "    <figcaption>Figure 1 - Diagram of Pipelines Architecture in Spacy. By <a href='https://course.spacy.io/en/chapter3'>Spacy - Advanced NLP with Spacy Course - Chapter 3</a>.</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure style='text-aling:center'>\n",
    "    <img style='border-radius:20px' src='./images/2.1-built-in-pipelines-of-text-processing.png' alt='Table of Pipelines Roles in Spacy' />\n",
    "    <figcaption>Figure 2 - Table of Pipelines's Roles in Spacy. By <a href='https://course.spacy.io/en/chapter3'>Spacy - Advanced NLP with Spacy Course - Chapter 3</a>.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0560db6-6f24-4a6a-a14d-1491e8e48d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing Pipelines Architecture\n",
    "import spacy\n",
    "\n",
    "nlp_sm = spacy.load('en_core_web_sm')\n",
    "nlp_md = spacy.load('en_core_web_md')\n",
    "nlp_lg = spacy.load('en_core_web_lg')\n",
    "nlp_trf = spacy.load('en_core_web_trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2e2d741-0935-4383-ae22-725fbe2bb504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Small Model Pipelines: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "- Medium Model Pipelines: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "- Large Model Pipelines: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "- TRF Model Pipelines: ['transformer', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# Listing Pipelines Architecture (List of Pipeline' Names)\n",
    "print(f'- Small Model Pipelines: {nlp_sm.pipe_names}')\n",
    "print(f'- Medium Model Pipelines: {nlp_md.pipe_names}')\n",
    "print(f'- Large Model Pipelines: {nlp_lg.pipe_names}')\n",
    "print(f'- TRF Model Pipelines: {nlp_trf.pipe_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "965a217a-c71b-4014-91e9-6581bd070963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Small Model Pipelines: [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x0000015BD6D40290>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x0000015BD6D40830>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x0000015BE0585CB0>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x0000015BB6BE16D0>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x0000015B97CF0610>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x0000015BB831A960>)]\n",
      "- Medium Model Pipelines: [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x0000015BF9E6C6B0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x0000015BD6D40650>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x0000015BF9CBD7E0>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x0000015BF99A1090>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x0000015BF999AE50>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x0000015BF9CBDA10>)]\n",
      "- Large Model Pipelines: [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x0000015B8EED09B0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x0000015B8EED0CB0>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x0000015B8633ACE0>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x0000015BFAD06850>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x0000015BFAC62590>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x0000015B8633B290>)]\n",
      "- TRF Model Pipelines: [('transformer', <spacy_curated_transformers.pipeline.transformer.CuratedTransformer object at 0x0000015B928E5550>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x0000015B928E58B0>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x0000015B943AC430>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x0000015BFAD0E2D0>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x0000015B8EF3EB50>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x0000015B943AC200>)]\n"
     ]
    }
   ],
   "source": [
    "# Listing Pipelines Architecture (Tuple of Pipeline' Names and Pipeline's Objects)\n",
    "print(f'- Small Model Pipelines: {nlp_sm.pipeline}')\n",
    "print(f'- Medium Model Pipelines: {nlp_md.pipeline}')\n",
    "print(f'- Large Model Pipelines: {nlp_lg.pipeline}')\n",
    "print(f'- TRF Model Pipelines: {nlp_trf.pipeline}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab72c851-91bb-406b-9d2f-02adec90205e",
   "metadata": {},
   "source": [
    "<h1 id='1-analysing-pipelines-problems' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>‚ùå | Analysing Pipelines Problems</h1>\n",
    "\n",
    "There's a possibility of occurring errors in some Pipelines when loading pre-trained models, so, in order to identify and fix them, we can do a scan in the model!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02675b8a-fd52-4afe-8818-a71f18b0a72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================= Pipeline Overview =============================\u001b[0m\n",
      "\n",
      "#   Component         Assigns               Requires   Scores             Retokenizes\n",
      "-   ---------------   -------------------   --------   ----------------   -----------\n",
      "0   tok2vec           doc.tensor                                          False      \n",
      "                                                                                     \n",
      "1   tagger            token.tag                        tag_acc            False      \n",
      "                                                                                     \n",
      "2   parser            token.dep                        dep_uas            False      \n",
      "                      token.head                       dep_las                       \n",
      "                      token.is_sent_start              dep_las_per_type              \n",
      "                      doc.sents                        sents_p                       \n",
      "                                                       sents_r                       \n",
      "                                                       sents_f                       \n",
      "                                                                                     \n",
      "3   attribute_ruler                                                       False      \n",
      "                                                                                     \n",
      "4   lemmatizer        token.lemma                      lemma_acc          False      \n",
      "                                                                                     \n",
      "5   ner               doc.ents                         ents_f             False      \n",
      "                      token.ent_iob                    ents_p                        \n",
      "                      token.ent_type                   ents_r                        \n",
      "                                                       ents_per_type                 \n",
      "\n",
      "\u001b[38;5;2m‚úî No problems found.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Analysing Pipelines Problems\n",
    "pipelines_analysis = nlp_lg.analyze_pipes(pretty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "014f8538-feae-4f5e-ae84-10b57b414492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Problems Found: {'tok2vec': [], 'tagger': [], 'parser': [], 'attribute_ruler': [], 'lemmatizer': [], 'ner': []}\n"
     ]
    }
   ],
   "source": [
    "# Analysing Pipelines Problems\n",
    "print(f'- Problems Found: {pipelines_analysis[\"problems\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b5ce432-3c21-45f7-9495-d0634326333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysing Pipelines Problems\n",
    "for pipeline_name, problems_list in pipelines_analysis['problems'].items():\n",
    "    assert len(problems_list) == 0, f'- Problems Found into {pipeline_name} Pipeline: {problems_list}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08700f72-8556-4ad9-98f3-1ab4b802363d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'ner': []},\n",
       " 'attrs': {'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.ents': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner'], 'requires': []}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analysing Pipelines Problems\n",
    "pipelines_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaf41be-d491-4d0b-b6ef-419f69ff39cc",
   "metadata": {},
   "source": [
    "<h1 id='2-new-pipelines' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>ü™à | New Pipelines</h1>\n",
    "\n",
    "There are two handy Pipelines that are not automatically loaded when using pre-trained models, `merge_noun_chunks` and `merge_entities`.\n",
    "\n",
    "- **merge_noun_chunk** - `sometimes we want to work with words formed by more than one Token, such as Nouns. For instance, consider the sentence: \"Let's head to the Kame House\", 'the Kame House' is considered as a single noun, but in Spacy Tokens, it's split into three Tokens. So, in order to check it out, we can access 'noun_chunks' attribute; and to merge this kind of nouns, we can add a new built-in Pipeline called 'merge_noun_chunks' at the end of our NLP object`;\n",
    "\n",
    "- **merge_entities** - `instead of merging every single noun chunk, we can focus on just merging Named Entities adding 'merge_entities' Pipeline`.\n",
    "\n",
    "Let's see some practical examples!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9902cc25-7dd3-4290-b432-2a4dcb693c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Token: Hey\n",
      "- Token: it\n",
      "- Token: 's\n",
      "- Token: me\n",
      "- Token: ,\n",
      "- Token: Goku\n",
      "- Token: !\n",
      "- Token: Let\n",
      "- Token: 's\n",
      "- Token: head\n",
      "- Token: to\n",
      "- Token: the\n",
      "- Token: Kame\n",
      "- Token: House\n",
      "- Token: !\n",
      "---\n",
      "- Noun Chunk: it\n",
      "- Noun Chunk: me\n",
      "- Noun Chunk: Goku\n",
      "- Noun Chunk: 's\n",
      "- Noun Chunk: the Kame House\n"
     ]
    }
   ],
   "source": [
    "# Merge Noun Chunk Pipeline\n",
    "document = nlp_lg('Hey it\\'s me, Goku! Let\\'s head to the Kame House!')\n",
    "\n",
    "for token in document: print(f'- Token: {token.text}')\n",
    "print('---')\n",
    "for noun_chunk in document.noun_chunks: print(f'- Noun Chunk: {noun_chunk}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e623ef9-bbdc-475d-8276-e064a2f78405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'tagger',\n",
       " 'parser',\n",
       " 'attribute_ruler',\n",
       " 'lemmatizer',\n",
       " 'ner',\n",
       " 'merge_noun_chunks']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge Noun Chunk Pipeline\n",
    "nlp_lg.add_pipe('merge_noun_chunks')\n",
    "nlp_lg.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de652282-77fb-4349-a0f6-f1eb9e7eabcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Token: Hey\n",
      "- Token: it\n",
      "- Token: 's\n",
      "- Token: me\n",
      "- Token: ,\n",
      "- Token: Goku\n",
      "- Token: !\n",
      "- Token: Let\n",
      "- Token: 's\n",
      "- Token: head\n",
      "- Token: to\n",
      "- Token: the Kame House\n",
      "- Token: !\n",
      "---\n",
      "- Noun Chunk: it\n",
      "- Noun Chunk: me\n",
      "- Noun Chunk: Goku\n",
      "- Noun Chunk: 's\n",
      "- Noun Chunk: the Kame House\n"
     ]
    }
   ],
   "source": [
    "# Merge Noun Chunk Pipeline\n",
    "document = nlp_lg('Hey it\\'s me, Goku! Let\\'s head to the Kame House!')\n",
    "for token in document: print(f'- Token: {token.text}')\n",
    "print('---')\n",
    "for noun_chunk in document.noun_chunks: print(f'- Noun Chunk: {noun_chunk}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a204d251-692a-4a3f-b15f-1521dd9f4d59",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b14c24f-ccd3-48fc-bf20-38e593d25a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'tagger',\n",
       " 'parser',\n",
       " 'attribute_ruler',\n",
       " 'lemmatizer',\n",
       " 'ner',\n",
       " 'merge_entities']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge Entities Pipeline\n",
    "nlp_lg.remove_pipe('merge_noun_chunks')\n",
    "nlp_lg.add_pipe('merge_entities')\n",
    "nlp_lg.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7854cef3-4a4d-4800-8516-3d5c6d5623b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Text: Hey\n",
      "- Text: it\n",
      "- Text: 's\n",
      "- Text: me\n",
      "- Text: ,\n",
      "- Text: Goku\n",
      "- Text: !\n",
      "- Text: Let\n",
      "- Text: 's\n",
      "- Text: head\n",
      "- Text: to\n",
      "- Text: the Kame House\n",
      "- Text: !\n",
      "---\n",
      "- Entity: Goku\n",
      "- Entity: the Kame House\n"
     ]
    }
   ],
   "source": [
    "# Merge Entities Pipeline\n",
    "document = nlp_lg('Hey it\\'s me, Goku! Let\\'s head to the Kame House!')\n",
    "for token in document: print(f'- Text: {token.text}')\n",
    "print('---')\n",
    "for entity in document.ents: print(f'- Entity: {entity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8bd9758-c2d4-46f5-b64c-dbc21fe72bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('merge_entities',\n",
       " <function spacy.pipeline.functions.merge_entities(doc: spacy.tokens.doc.Doc)>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge Entities Pipeline\n",
    "nlp_lg.remove_pipe('merge_entities')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbf372b-26c7-4325-92b7-4e83d2db2e89",
   "metadata": {},
   "source": [
    "<h1 id='3-custom-pipeline-components' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>üé® | Custom Pipeline Components</h1>\n",
    "\n",
    "Spacy allows us to create and add our own functions into its Pipeline Architecture in order to add specific rules and logic to process texts. With this, we are able to do a variety of things, such as, modify the Document, add more data to Tokens and even update the Named Entities (NER) list.\n",
    "\n",
    "These added functions are considered as `Custom Pipeline Components`, also known as `Components`, and they are Pipelines that must modify and return a Document. Besides, in order to be considered as a Component, we must add `@Language.component` decorator before defining the function.\n",
    "\n",
    "After creating our Component, we are able to add it into Model's Pipeline Architecture. When adding it, we can specify one of the four available positions:\n",
    "\n",
    "- **last (default behaviour)** - `if True, the Component will be added in the end of the architecture`;\n",
    "  \n",
    "- **first** - `if True, the Component will be added in the beginning of the architecture, right after the Tokenizer Pipeline`;\n",
    "  \n",
    "- **before** - `the Component will be added right before the specified Pipeline`;\n",
    "\n",
    "- **after** - `the Component will be added right after the specified Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f9d9573-5c64-41b0-b084-7fd14336c8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Pipelines: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'custom_component']\n",
      "- Document Length: 7\n"
     ]
    }
   ],
   "source": [
    "# Custom Pipeline Components\n",
    "from spacy.language import Language\n",
    "\n",
    "@Language.component('custom_component')\n",
    "def length_component(document):\n",
    "    print(f'- Document Length: {len(document)}')\n",
    "    return document\n",
    "\n",
    "nlp_lg.add_pipe('custom_component') # default behavior: last=True\n",
    "print(f'- Pipelines: {nlp_lg.pipe_names}')\n",
    "\n",
    "document = nlp_lg('Hey it\\'s me, Goku!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4adfb10-af09-4e8d-99dd-0ee827f9448c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fbed701-0cdf-4895-ba1f-7b6f687a2d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Entity Text: Golden Retriever\n",
      "- Entity Label: ANIMAL\n",
      "- Label Explanation: None\n",
      "---\n",
      "- Entity Text: cat\n",
      "- Entity Label: ANIMAL\n",
      "- Label Explanation: None\n",
      "---\n",
      "- Entity Text: turtle\n",
      "- Entity Label: ANIMAL\n",
      "- Label Explanation: None\n",
      "---\n",
      "- Entity Text: bunny\n",
      "- Entity Label: ANIMAL\n",
      "- Label Explanation: None\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\spacy\\glossary.py:20: UserWarning: [W118] Term 'ANIMAL' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
      "  warnings.warn(Warnings.W118.format(term=term))\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1) Create a Custom Component to search for Animals and then\n",
    "# update the Document Entities in order to contain only the matched\n",
    "# Animals\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp_large = spacy.load('en_core_web_lg')\n",
    "\n",
    "animals = ['Golden Retriever', 'cat', 'turtle', 'bunny']\n",
    "animals_pattern = nlp_large.pipe(animals)\n",
    "\n",
    "matcher = PhraseMatcher(nlp_large.vocab)\n",
    "matcher.add('ANIMALS', animals_pattern)\n",
    "\n",
    "@Language.component('animals_component')\n",
    "def animals_component(document):\n",
    "    matches = matcher(document)\n",
    "    spans = [\n",
    "        Span(document, start_index, end_index, label='ANIMAL')\n",
    "        for match_id, start_index, end_index in matches\n",
    "    ]\n",
    "    document.ents = spans\n",
    "    return document\n",
    "\n",
    "nlp_large.add_pipe('animals_component', after='ner')\n",
    "\n",
    "document = nlp_large('I have a Golden Retriever, also a cat, a turtle and a little bunny.')\n",
    "\n",
    "for entity in document.ents:\n",
    "    print(f'- Entity Text: {entity.text}')\n",
    "    print(f'- Entity Label: {entity.label_}')\n",
    "    print(f'- Label Explanation: {spacy.explain(entity.label_)}')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16a59e6-39ba-48e2-b25c-54941603eacd",
   "metadata": {},
   "source": [
    "<h1 id='4-extension-types-attributes-properties-and-methods' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>üß© | Extension Types: Attributes, Properties and Methods</h1>\n",
    "\n",
    "Now let's see how we can add custom `Attributes, Properties and Methods` into Documents, Tokens and Spans. All custom extensions added into them are accessible with '_.', telling that the Attribute/Property/Method being accessed is a custom one created by us instead of by Spacy.\n",
    "\n",
    "The three types of Custom Extensions are:\n",
    "\n",
    "- **Attributes** - `variables with a default value set to it and allow us to overwrite their values`;\n",
    "  \n",
    "- **Properties** - `functions that automatically sets a value to Attributes accordingly to our custom logic. We can define a required 'getter' and an optional 'setter'`;\n",
    "  \n",
    "- **Methods** - `functions that returns a value accordingly to our custom logic. They don't create an accessible Attribute, but yes, an accessible Function`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "272382fd-b064-4619-ada4-088ece46fada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc, Token, Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92757f88-3bc7-45b2-b82f-f8f450f33bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribute Extensions\n",
    "Doc.set_extension('title', default=None)\n",
    "Token.set_extension('is_color', default=False)\n",
    "Span.set_extension('has_color', default=False)\n",
    "\n",
    "document = nlp_large('My favorite colors are purple, black and gold!')\n",
    "token = document[4]\n",
    "span = document[0:3]\n",
    "\n",
    "document._.title = 'Favorite Colors'\n",
    "token._.is_color = True\n",
    "span._.has_color = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae2def3f-c9a4-4d93-a91b-1dee198bf3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Is Token a favorite color? True - purple\n"
     ]
    }
   ],
   "source": [
    "# Property Extensions - Tokens\n",
    "def get_is_favorite_color(token):\n",
    "    colors = ['purple', 'black', 'gold']\n",
    "    return token.text in colors\n",
    "\n",
    "Token.set_extension('is_favorite_color', getter=get_is_favorite_color)\n",
    "token = document[4]\n",
    "print(f'- Is Token a favorite color? {token._.is_favorite_color} - {token.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5de1437-d678-4e9d-a4b9-3859b5aae0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does Span 1 contain a favorite color? True - My favorite colors are purple,\n",
      "Does Span 2 contain a favorite color? True - and gold!\n"
     ]
    }
   ],
   "source": [
    "# Propety Extensions - Spans\n",
    "def get_has_favorite_color(span):\n",
    "    colors = ['purple', 'black', 'gold']\n",
    "    return any(token.text in colors for token in span)\n",
    "\n",
    "Span.set_extension('has_favorite_color', getter=get_has_favorite_color)\n",
    "span1 = document[0:6]\n",
    "span2 = document[7:10]\n",
    "print(f'Does Span 1 contain a favorite color? {span1._.has_favorite_color} - {span1.text}')\n",
    "print(f'Does Span 2 contain a favorite color? {span2._.has_favorite_color} - {span2.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c527a7d0-b168-45dd-a482-621e0848c69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does Document contain the purple color? True\n",
      "Does Document contain the red color? False\n"
     ]
    }
   ],
   "source": [
    "# Method Extensions\n",
    "def has_token(document, token_text):\n",
    "    return token_text in [token.text for token in document]\n",
    "\n",
    "Doc.set_extension('has_token', method=has_token)\n",
    "print(f'Does Document contain the purple color? {document._.has_token(\"purple\")}')\n",
    "print(f'Does Document contain the red color? {document._.has_token(\"red\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c0dda2-7de6-494c-9e95-8c0f4bdf00cf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1b597b9-f193-4c34-979b-4370bd9dd513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- fifty years: None\n",
      "- first: None\n",
      "- David Bowie: https://en.wikipedia.org/w/index.php?search=David_Bowie\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2) Generate Wikipedia URLs for Person, Organization,\n",
    "# Country and Location Spans\n",
    "nlp_large = spacy.load('en_core_web_lg')\n",
    "\n",
    "def get_wikipedia_url(span):\n",
    "    if span.label_ in ['PERSON', 'ORG', 'GPE', 'LOCATION']:\n",
    "        entity_text = span.text.replace(' ', '_')\n",
    "        return 'https://en.wikipedia.org/w/index.php?search=' + entity_text\n",
    "\n",
    "Span.set_extension('wikipedia_url', getter=get_wikipedia_url)\n",
    "\n",
    "document = nlp_large(\n",
    "    'In over fifty years from his very first recordings right through to his '\n",
    "    'last album, David Bowie was at the vanguard of contemporary culture.'\n",
    ")\n",
    "\n",
    "for entity in document.ents: print(f'- {entity.text}: {entity._.wikipedia_url}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c9db03-ad3f-4599-9fc4-acbbd8cb235e",
   "metadata": {},
   "source": [
    "<h1 id='5-scaling-and-performance' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>üìà | Scaling and Performance</h1>\n",
    "\n",
    "Now, let's see some tips to boost our NLP tasks!!\n",
    "\n",
    "```\n",
    "- nlp.pipe\n",
    "- Passing in Context\n",
    "- Using only the Tokenizer\n",
    "- Disabling Pipeline Components\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c83112-c53c-4bc1-9bf2-bb821929de07",
   "metadata": {},
   "source": [
    "- **nlp.pipe** - `useful when processing multiple phrases and yielding multiple Documents`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ac14482-f15f-4551-a953-d417b20bf0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp.pipe\n",
    "texts = ['Hey it\\'s me, Goku!', 'You look strong, let\\'s fight!']\n",
    "\n",
    "# Bad Way\n",
    "documents1 = [nlp_large(text) for text in texts]\n",
    "\n",
    "# Good Way\n",
    "documents2 = list(nlp_large.pipe(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caecfbcb-26dd-4610-bdd4-fa23845aa27a",
   "metadata": {},
   "source": [
    "- **Passing in Context (Part I)** - `when passing 'as_tuples' parameter as 'True' into 'nlp.pipe', we can add additional metadata to the Document`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2b8264c-5191-4751-b9ca-c6035330c035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Document: Hey it's me, Goku!\n",
      "- Context ID: 1\n",
      "- Context Page Number: 7\n",
      "---\n",
      "- Document: You look strong, let's fight!\n",
      "- Context ID: 2\n",
      "- Context Page Number: 8\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Passing in Context (1)\n",
    "texts = [\n",
    "    ('Hey it\\'s me, Goku!', { 'id': 1, 'page_number': 7 })\n",
    "    , ('You look strong, let\\'s fight!', { 'id': 2, 'page_number': 8 })\n",
    "]\n",
    "\n",
    "for document, context in nlp_large.pipe(texts, as_tuples=True):\n",
    "    print(f'- Document: {document.text}')\n",
    "    print(f'- Context ID: {context[\"id\"]}')\n",
    "    print(f'- Context Page Number: {context[\"page_number\"]}')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e703ba-db16-483f-bc4a-824d755c7e5b",
   "metadata": {},
   "source": [
    "- **Passing in Context (Part II)** - `when working with Attribute and Property Extensions, Context may be handy to add values to the Extensions`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "722ff741-fd2a-4088-bbf5-fde6f9ffaaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Document's ID: 1\n",
      "- Document's Page Number: 7\n",
      "---\n",
      "- Document's ID: 2\n",
      "- Document's Page Number: 8\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Passing in Context (2)\n",
    "Doc.set_extension('id', default=None)\n",
    "Doc.set_extension('page_number', default=None)\n",
    "\n",
    "texts = [\n",
    "    ('Hey it\\'s me, Goku!', { 'id': 1, 'page_number': 7 })\n",
    "    , ('You look strong, let\\'s fight!', { 'id': 2, 'page_number': 8 })\n",
    "]\n",
    "\n",
    "for document, context in nlp_large.pipe(texts, as_tuples=True):\n",
    "    document._.id = context['id']\n",
    "    document._.page_number = context['page_number']\n",
    "    print(f'- Document\\'s ID: {document._.id}')\n",
    "    print(f'- Document\\'s Page Number: {document._.page_number}')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2fc928-b5a7-49b5-9739-91de91cd5193",
   "metadata": {},
   "source": [
    "- **Using only the Tokenizer** - `sometimes we desire to just use the Tokenizer Pipeline on Documents, skipping all the other ones, such as Tok2Vec, Tagger, Parser, NER and Lemmatizer`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9339ebd-8581-4c5b-b73c-fd70a3a6032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing Text with All Pipelines\n",
    "document1 = nlp_large('Hey it\\'s me, Goku!')\n",
    "\n",
    "# Processing Text with Tokenizer Only\n",
    "document2 = nlp_large.make_doc('Hey it\\'s me, Goku!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e350fce9-e77a-4eda-9ea4-25f13eff539a",
   "metadata": {},
   "source": [
    "- **Disabling Pipeline Components** - `other times we desire to process text with Tokenizer and some specific Pipelines, such as Tagger only`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3ac0a33-512d-4236-a7d2-31af3b0629c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "# Running Tokenizer and Tagger and Lemmatizer Only\n",
    "with nlp_large.select_pipes(enable=['tagger', 'lemmatizer']):\n",
    "    document1 = nlp_large('Hey it\\'s me, Goku!')\n",
    "\n",
    "# Running All Pipelines, but Tagger and Lemmatizer\n",
    "with nlp_large.select_pipes(disable=['tagger', 'lemmatizer']):\n",
    "    document2 = nlp_large('Hey it\\'s me, Goku!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296dac73-17ce-4965-815c-9d3b9c571869",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h1 id='reach-me' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>üì´ | Reach Me</h1>\n",
    "\n",
    "> **Email** - [csfelix08@gmail.com](mailto:csfelix08@gmail.com?)\n",
    "\n",
    "> **Linkedin** - [linkedin.com/in/csfelix/](https://www.linkedin.com/in/csfelix/)\n",
    "\n",
    "> **GitHub:** - [CSFelix](https://github.com/CSFelix)\n",
    "\n",
    "> **Kaggle** - [DSFelix](https://www.kaggle.com/dsfelix)\n",
    "\n",
    "> **Portfolio** - [CSFelix.io](https://csfelix.github.io/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57fe8e3a-0a67-457e-ac75-555c4c0afd93",
   "metadata": {},
   "source": [
    "<center>    \n",
    "    <h1 id='spacy-notebook-10' style='color:#7159c1; font-size:350%'>Evaluations</h1>\n",
    "    <i style='font-size:125%'>Exploring Evaluations Strategies and Kappa's Score</i>\n",
    "</center>\n",
    "\n",
    "> **Topics**\n",
    "\n",
    "```\n",
    "- ðŸª™ Gold Standard\n",
    "- ðŸŽ¯ Accuracy\n",
    "- ðŸ¥… Cohen's Kappa Score\n",
    "- ðŸ¥… Fleiss's Kappa Score\n",
    "- ðŸ¥… Weighted Cohen's Kappa Score\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20df0332-241d-4d26-90c0-9d4a4e5fce6b",
   "metadata": {},
   "source": [
    "<h1 id='0-gold-standard' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>ðŸª™ | Gold Standard</h1>\n",
    "\n",
    "Suppose that we created a Language Model (LM) that predicts the sentiment related to anime comments as `positive`, `neutral` and `negative`.\n",
    "\n",
    "In order to evaluate how good the predictions were, we must compare them to a dataset containing the correct sentiments related to each comment. This very dataset is called `Gold Standard`, so:\n",
    "\n",
    "```txt\n",
    "Validation or Evaluation Dataset == Gold Standard\n",
    "```\n",
    "\n",
    "For now, let's suppose our `Gold Standard` and model's predictions are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac6e6f33-fcbd-408e-be4c-783a05b8ebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_standard = [\n",
    "    'positive', 'neutral', 'negative', 'positive', 'neutral'\n",
    "    , 'positive', 'negative', 'neutral', 'positive', 'negative'\n",
    "]\n",
    "\n",
    "model_predictions = [\n",
    "    'positive', 'neutral', 'negative', 'neutral', 'neutral'\n",
    "    , 'positive', 'positive', 'neutral', 'positive', 'neutral'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81a6154-da0f-4662-ba4f-f4deb3d11f3d",
   "metadata": {},
   "source": [
    "<h1 id='1-accuracy' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>ðŸŽ¯ | Accuracy</h1>\n",
    "\n",
    "Taking the Gold Standard and model's predictions into consideration, the first thing we could think is to calculate the `Accuracy` between them, that is, how many predictions matched the correct sentiment related to the anime comments. This metric is usually known as `Accuracy` or `Agreement Degree`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "738115e7-9b7c-432b-8280-8ffb321af1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 out of 10 comments have been correctly classified!!\n",
    "agreement_degree = 7 / 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c93f07f-8a5f-4663-a778-de708565803a",
   "metadata": {},
   "source": [
    "<h1 id='2-cohens-kappa-score' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>ðŸ¥… | Cohen's Kappa Score</h1>\n",
    "\n",
    "Even though Accuracy (Agreement Degree) considers how many predictions were done correctly by the model; relying only on it can lead us to terrible conclusions, since it totally ignores the probability of the model got correct predictions `by chance`, like tossing a die to make a prediction as 'positive', 'neutral' or 'negative' and, consequently, biasing the Agreement percentage.\n",
    "\n",
    "In order to avoid it, we can use `Cohen's Kappa` score, that can be used when:\n",
    "\n",
    "- we are evaluating `two raters`, e.g. Gold Standard and model's predictions;\n",
    "\n",
    "- the categores (prediction results) are `nominal`, that is, they are not ordinal, so when `there isn't hierarchy between them`.\n",
    "\n",
    "It's equation is given by:\n",
    "\n",
    "```python\n",
    "cohens_kappa = (observed_probability - expected_probability) / (1 - expected_probability)\n",
    "```\n",
    "\n",
    "$$\n",
    "\\text{Cohen's Kappa} = \\frac{(\\text{observed probability} - \\text{expected probability})}{(1 - \\text{expected probability})}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- **observed probability** - `probability of the observed/obtained predictions`;\n",
    "\n",
    "-  **expected probability** - `probability of all predictions were made by chance`.\n",
    "\n",
    "---\n",
    "\n",
    "The `Observed Probability (Po)` is literally the `Accuracy (Agreement Degree)`, whereas the `Expected Probability (Pe)` consists of the probability of the Gold Standard and the model's predictions be each one of the available categories\\sentiments, so:\n",
    "\n",
    "```python\n",
    "observed_probability = correct_predictions / total_predictions\n",
    "expected_probability = sum(prod(frequency_category / total_predictions))\n",
    "```\n",
    "\n",
    "$$\n",
    "\\text{Observed Probability} = \\frac{\\text{correctPredictions}}{\\text{totalPredictions}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Expected Probability} = \\sum_{i}{(\\prod_{j}({\\frac{\\text{frequencyCategory}_{i,j}}{\\text{totalPredictions}})})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35f36504-1629-4090-9247-e0684769831a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Observed Probability (Po): 0.7\n"
     ]
    }
   ],
   "source": [
    "# Observed Probability\n",
    "observed_probability = 7 / 10\n",
    "print(f'- Observed Probability (Po): {observed_probability}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0672fad7-e152-4a6f-82ce-926df213de81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Expected Probability (Pe): 0.3400000000000001\n"
     ]
    }
   ],
   "source": [
    "# Expected Probability\n",
    "#\n",
    "#  gold_standard_sentiment_probability * model_prediction_sentiment_probability\n",
    "#\n",
    "positive_probability = (4 / 10) * (4 / 10)\n",
    "neutral_probability = (3 / 10) * (5 / 10)\n",
    "negative_probability = (3 / 10) * (1 / 10)\n",
    "\n",
    "expected_probability = positive_probability + neutral_probability + negative_probability\n",
    "print(f'- Expected Probability (Pe): {expected_probability}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50bb398-2cc1-47c3-83cc-7c0362b7d706",
   "metadata": {},
   "source": [
    "With both Observed and Expected Probabilities, we can finally calculate `Cohen's Kappa` score. To do it, we just need to subtract the randomness probability from the observed one and then divide the result by the complement of the randomness probability in order to scale the score in a range from -1 to +1.\n",
    "\n",
    "```python\n",
    "cohens_kappa = (observed_probability - expected_probability) / (1 - expected_probability)\n",
    "```\n",
    "\n",
    "$$\n",
    "\\text{Cohen's Kappa} = \\frac{(\\text{observed probability} - \\text{expected probability})}{(1 - \\text{expected probability})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6665d8e4-2165-4a6d-8031-eeacfbf886c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Cohen's Kappa Score: 0.5454545454545453\n"
     ]
    }
   ],
   "source": [
    "# Cohen's Kappa Score\n",
    "cohens_kappa_score = (observed_probability - expected_probability) / (1 - expected_probability)\n",
    "print(f'- Cohen\\'s Kappa Score: {cohens_kappa_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7592b2-1f76-4f49-a8ba-0f5ea7fc5b81",
   "metadata": {},
   "source": [
    "There's also a table proposed by Landis and Koch to interpret Cohen's Kappa score. Since the divisions were created `arbitrary`, we must take it with a pinch of salt!!\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Score</th>\n",
    "        <th>Strength of Agreement</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>< 0.00</td>\n",
    "        <td>Poor</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0.00 to 0.20</td>\n",
    "        <td>Slight</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0.21 to 0.40</td>\n",
    "        <td>Fair</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0.41 to 0.60</td>\n",
    "        <td>Moderate</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0.61 to 0.80</td>\n",
    "        <td>Substantial</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0.81 to 1.00</td>\n",
    "        <td>Perfect</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Taking our example into consideration, since Cohen's Kappa score is approximately 0.54, we can tell that the model's predictions were `Moderate` reliable to the Gold Standard!!\n",
    "\n",
    "---\n",
    "\n",
    "It's obvious that in a real world project, we won't be calculating Cohen's Kappa score by hand when we can get advantage of a great Python's Package to do the job for us.\n",
    "\n",
    "We only calculated by hand in order to get the glimpse of how the algorithm works.\n",
    "\n",
    "Now let's do it, but using `sklearn`!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3d99c5c-6610-4795-9a49-d67d5f536635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Cohen's Kappa Score from SKLearn: 0.5454545454545454\n"
     ]
    }
   ],
   "source": [
    "# Calculating Cohen's Kappa Score with SKLearn\n",
    "#\n",
    "#  OBS.: the result can differ a few decimals due to roundings\n",
    "#\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "cohen_kappa_score_sklearn = cohen_kappa_score(gold_standard, model_predictions)\n",
    "print(f'- Cohen\\'s Kappa Score from SKLearn: {cohen_kappa_score_sklearn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1bab3c-5936-4f6a-9e92-ea4e05a06f66",
   "metadata": {},
   "source": [
    "<h1 id='2-fleiss-kappa-score' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>ðŸ¥… | Fleiss's Kappa Score</h1>\n",
    "\n",
    "Cohen's Kappa score is a good metric when we are evaluating only two raters, e.g. Gold Standard and a model's prediction, but when it comes to three or more raters, we should do some adjustments in order to get accurate results. So, when we are evaluating `more than two raters`, we must stick to `Fleiss's Kappa` score, so:\n",
    "\n",
    "- we are evaluating `three or more raters`, e.g. Gold Standard, model 1's predictions and model 2's predictions;\n",
    "\n",
    "- the categories are `nominal`, that is, they are not ordinal and `there isn't hierarchy between them`.\n",
    "\n",
    "For the calculations, the major equation is the same:\n",
    "\n",
    "```python\n",
    "fleiss_kappa = (observed_probability - expected_probability) / (1 - expected_probability)\n",
    "```\n",
    "\n",
    "$$\n",
    "\\text{Fleiss's Kappa} = \\frac{(\\text{observed probability} - \\text{expected probability})}{(1 - \\text{expected probability})}\n",
    "$$\n",
    "\n",
    "The unique differences are how the `Observed Probability (Po)` and the `Expected Probability (Pe)` are calculated!!\n",
    "\n",
    "So, let's consider the same Gold Standard amd model's predictions from the previous example, but adding a new list of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d05e5c32-fd28-4893-a3c1-365d5f6e7143",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_standard = [\n",
    "  'positive', 'neutral', 'negative', 'positive', 'neutral'\n",
    "  , 'positive', 'negative', 'neutral', 'positive', 'negative'\n",
    "]\n",
    "\n",
    "model_1_predictions = [\n",
    "  'positive', 'neutral', 'negative', 'neutral', 'neutral'\n",
    "  , 'positive', 'positive', 'neutral', 'positive', 'neutral'\n",
    "]\n",
    "\n",
    "model_2_predictions = [\n",
    "  'neutral', 'neutral', 'negative', 'positive', 'positive'\n",
    "  , 'positive', 'negative', 'neutral', 'neutral', 'negative'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18382b6c-73f5-4b64-b119-4c10a1fddc96",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's calculate the `Expected Probability (Pe)` first, so we first need to create a table where the index is each comment, the columns are each rater and therows are the assigned sentiment by the rater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f0cfb02-4399-43c1-aaa2-d88e2b921383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gold_standard</th>\n",
       "      <th>model_1_predictions</th>\n",
       "      <th>model_2_predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gold_standard model_1_predictions model_2_predictions\n",
       "0      positive            positive             neutral\n",
       "1       neutral             neutral             neutral\n",
       "2      negative            negative            negative\n",
       "3      positive             neutral            positive\n",
       "4       neutral             neutral            positive\n",
       "5      positive            positive            positive\n",
       "6      negative            positive            negative\n",
       "7       neutral             neutral             neutral\n",
       "8      positive            positive             neutral\n",
       "9      negative             neutral            negative"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expected Probability\n",
    "import pandas as pd\n",
    "\n",
    "evaluation_table = pd.DataFrame(columns=['gold_standard', 'model_1_predictions', 'model_2_predictions'])\n",
    "evaluation_table['gold_standard'] = gold_standard\n",
    "evaluation_table['model_1_predictions'] = model_1_predictions\n",
    "evaluation_table['model_2_predictions'] = model_2_predictions\n",
    "evaluation_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa501c8-6353-43c7-abc5-a7caf02653c9",
   "metadata": {},
   "source": [
    "After this, we create a second table where the index is each comment, the columns are each possible sentiment and the rows are the frequencies of the assigned sentiments to the comment.\n",
    "\n",
    "Then, we calculate the probability of each possible category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe00dde8-fb16-4202-9f43-c6b8a424e2e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   negative  neutral  positive\n",
       "0         0        1         2\n",
       "1         0        3         0\n",
       "2         3        0         0\n",
       "3         0        1         2\n",
       "4         0        2         1\n",
       "5         0        0         3\n",
       "6         2        0         1\n",
       "7         0        3         0\n",
       "8         0        1         2\n",
       "9         2        1         0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expected Probability\n",
    "frequency_table = evaluation_table.apply(\n",
    "    pd.Series.value_counts, axis=1\n",
    ")               \\\n",
    "  .fillna(0)    \\\n",
    "  .astype(int)\n",
    "\n",
    "frequency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11a1d707-3a19-4948-bde6-5416d8b171fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sum = frequency_table.sum().sum()\n",
    "positive_probability = frequency_table['positive'].sum() / total_sum\n",
    "neutral_probability = frequency_table['neutral'].sum() / total_sum\n",
    "negative_probability = frequency_table['negative'].sum() / total_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654fd67f-36a1-44b3-8b9f-dacfb855bdf4",
   "metadata": {},
   "source": [
    "Finally, we can get the `Expected Probability (Pe)` by getting the ratio of each sentiment and the sum their squares:\n",
    "\n",
    "```python\n",
    "predicted_probability = sum(expected_ratio**2)\n",
    "```\n",
    "\n",
    "$$\n",
    "\\text{Predicted Probability} = \\sum_{i}{(\\text{expectedRatio}_{i}^2)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c186900-dea1-46ae-8ad1-4f1ec0ef9c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Expected Probability: 0.3400000000000001\n"
     ]
    }
   ],
   "source": [
    "predicted_probability = (positive_probability**2) + (neutral_probability**2) + (negative_probability**2)\n",
    "print(f'- Expected Probability: {expected_probability}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507ab49e-6720-47df-83d0-74444e73b4c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now, to calculate the `Observed Probability (Po)`, we must resolve the following equation:\n",
    "\n",
    "```python\n",
    "observed_probability = (1 / (N * n * (n - 1))) * (sum(sum(nij**2))) - (N - n)\n",
    "```\n",
    "\n",
    "$$\n",
    "\\text{Observed Probability} = (\\frac{1}{N \\cdot n \\cdot (n - 1)}) \\cdot (\\sum_{i}^{N}{\\sum_{j}^{k}{n_{ij}^2}}) - (N \\cdot n)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- **N** - `number of items`;\n",
    "\n",
    "- **n** - `number of raters`;\n",
    "\n",
    "- **i** - `row index`;\n",
    "\n",
    "- **j** - `column index`;\n",
    "\n",
    "- **k** - `category`.\n",
    "\n",
    "Don't worry about the equation being big, let's just split it up into three small pieces and then solve it:\n",
    "\n",
    "$$\n",
    "\\text{Piece 1} = (\\frac{1}{N \\cdot n \\cdot (n - 1)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Piece 2} = \\sum_{i}^{n}{\\sum_{j}^{k}{n_{ij}^2}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Piece 3} = (N \\cdot n)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8d75a84-18ac-4d2c-b0af-dfb88082113c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Observed Probability: 0.6\n"
     ]
    }
   ],
   "source": [
    "# Observed Probability\n",
    "number_of_items = frequency_table.shape[0]\n",
    "number_of_raters = frequency_table.shape[1]\n",
    "\n",
    "piece_1 = 1 / (number_of_items * number_of_raters * (number_of_raters - 1))\n",
    "piece_2 = frequency_table.apply(lambda frequency: sum(frequency**2)).sum()\n",
    "piece_3 = number_of_items * number_of_raters\n",
    "\n",
    "observed_probability = piece_1 * (piece_2 - piece_3)\n",
    "print(f'- Observed Probability: {observed_probability}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dea3eba-8bf1-4423-b967-a53689930537",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now, to calculat `Fleiss's Kappa` score, we use the same equation from Cohen's Kappa score:\n",
    "\n",
    "```python\n",
    "fleiss_kappa = (observed_probability - expected_probability) / (1 - expected_probabilty)\n",
    "```\n",
    "\n",
    "$$\n",
    "\\text{Fleiss's Kappa} = \\frac{\\text{observedProbability} - \\text{expectedProbability}}{1 - \\text{expectedProbability}}\n",
    "$$\n",
    "\n",
    "So, realize that the equation is the same, being the way to calculate both observed and expected probabilities the unique differences!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "791cb3ab-a852-4479-887a-0bdc0c571094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fleiss's Kappa Score: 0.3939393939393938\n"
     ]
    }
   ],
   "source": [
    "# Fleiss's Kappa Score\n",
    "fleiss_kappa_score = (observed_probability - expected_probability) / (1 - expected_probability)\n",
    "print(f'- Fleiss\\'s Kappa Score: {fleiss_kappa_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2536350b-1b30-48e9-a525-9fdc6db60514",
   "metadata": {},
   "source": [
    "Taking our example into consideration, since Fleiss score is approximately 0.39, we can tell that the model's predictions were `Fair` reliable to the Gold Standard!!\n",
    "\n",
    "Now let's do the easy way to calculate the score using sklearn!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6923027a-4a1a-417c-9c34-7d25282a4664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fleiss's Kappa Score: 0.3856655290102387\n"
     ]
    }
   ],
   "source": [
    "# Calculating Fleiss's Kappa Score with SKLearn\n",
    "#\n",
    "#  OBS.: the result can differ a few decimals due to roundings\n",
    "#\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa\n",
    "fleiss_kappa_score = fleiss_kappa(frequency_table, method='fleiss')\n",
    "print(f'- Fleiss\\'s Kappa Score: {fleiss_kappa_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8e1e3f-20a1-46ae-9830-56b5e1852cfe",
   "metadata": {},
   "source": [
    "<h1 id='4-weighted-cohens-kappa-score' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>ðŸ¥… | Weighted Cohen's Kappa Score</h1>\n",
    "\n",
    "Sometimes, some categories can be more important than other ones and we must assign weights to them in order to express their importance degree. In this scenario, we must apply `Weighted Cohen's Kappa` score, specially when:\n",
    "\n",
    "- we are evaluating `two raters`, e.g. Gold Standard and model's predictions;\n",
    "\n",
    "- the categories are `ordinal`, that is, they are not nominal and `there's hierarchy between them`.\n",
    "\n",
    "For instance, taking our animes comment sentiments analysis example into consideration, we can say that the 'positive' sentiment is the most important one, 'neutral' the second, and 'negative the last one, then:\n",
    "\n",
    "$$\n",
    "\\text{positive} >> \\text{neutral} >> \\text{negative}\n",
    "$$\n",
    "\n",
    "And, to calculate the `Weighted Cohen's Kappa` score, we simply calculate the complement of the sum of the observed weights multiplied by the observed frequencies or probabilities, divided by the sum of the expected weights multiplied by the expected frequencies or probabilities:\n",
    "\n",
    "```python\n",
    "weighted_cohens_kappa = 1 - (sum(observed_weights * observed_probability)) / (sum(expected_weights * expected_probability))\n",
    "```\n",
    "\n",
    "$$\n",
    "\\text{Weighted Cohen's Kappa} = 1 - \\frac{\\sum_{i}({\\text{observedWeight}_{i} \\cdot \\text{observedProbability}_{i}})}{\\sum_{j}({\\text{expectedWeights}_{j} \\cdot \\text{expectedProbability}_{j}})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4f7bc03-caed-4b64-a2b5-3918142ab99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_standard = [\n",
    "\t'positive', 'neutral', 'negative', 'positive', 'neutral'\n",
    "\t, 'positive', 'negative', 'neutral', 'positive', 'negative'\n",
    "]\n",
    "\n",
    "model_predictions = [\n",
    "\t'positive', 'neutral', 'negative', 'neutral', 'neutral'\n",
    "\t, 'positive', 'positive', 'neutral', 'positive', 'neutral'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8142d66a-166b-4a02-ad63-32681fadcf3b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "First off, we have to calculate a `Matrix Confusion` between the Gold Standard and the model's predictions in order to get the `Observed Probability (Po)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a03ab90a-ea51-48f3-9fa3-adcee56c934e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1, 0.1, 0.1],\n",
       "       [0. , 0.3, 0. ],\n",
       "       [0. , 0.1, 0.3]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observed Probability\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "observed_frequency = confusion_matrix(gold_standard, model_predictions)\n",
    "total_frequency = observed_frequency.sum()\n",
    "observed_probability = observed_frequency / total_frequency\n",
    "observed_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ad6dc6-6ca8-45b2-8e4b-29de8ec06ba6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "For the `Expected Probability (Pe)`, we calculate the sum of each column and row and multiply the sum for each respective column and row index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d0d633e-1000-447d-8da0-a4544a5a666d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03, 0.15, 0.12],\n",
       "       [0.03, 0.15, 0.12],\n",
       "       [0.04, 0.2 , 0.16]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expected Probability\n",
    "import numpy as np\n",
    "\n",
    "rows_sum = observed_probability.sum(axis=1)\n",
    "columns_sum = observed_probability.sum(axis=0)\n",
    "\n",
    "expected_probability = np.outer(rows_sum, columns_sum)\n",
    "expected_frequency = expected_probability * total_frequency\n",
    "expected_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9c1f7b-4fbf-4a9b-a41a-9cefa6fd1e3d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now we can satar calculating the `weights`. There are two types of weights, the `Linear` and the `Quadratic`, whereas the first one apply the `same penalty` to all categories, the second one apply `small penalties` to the first ones and `big penalties` to the last ones. Then:\n",
    "\n",
    "- **Linear Weight** - `same weight gap between all categories, that is, the weight difference between the categories is the same`;\n",
    "\n",
    "- **Quadratic Weight** - `different weight gap between the categories, that is, the weight difference between the categories is different and increases accordingly the importance degree decreases`.\n",
    "\n",
    "Starting off `Linear Weight`, it's equation is given by the absolute value of the subtraction of the row and column indexes divided by the total number of categories minus 1:\n",
    "\n",
    "```python\n",
    "linear_weight = abs(row_index - column_index) / (total_categories - 1)\n",
    "```\n",
    "\n",
    "$$\n",
    "\\text{Linear Weight} = \\frac{ || \\text{rowIndex} - \\text{columnIndex} || }{\\text{totalCategories} - 1}\n",
    "$$\n",
    "\n",
    "Whereas `Quadratic Weight` is given by the square of the sum of row and column indexes divided by the sqyare of the total number of categories minus 1:\n",
    "\n",
    "```python\n",
    "quadratic_weight = ((row_index - column_index)**2) / ((total_categories - 1)**2)\n",
    "```\n",
    "\n",
    "$$\n",
    "\\text{Quadratic Weight} = \\frac{(\\text{rowIndex} - {columnIndex})^2}{(\\text{totalCategories} - 1)^2}\n",
    "$$\n",
    "\n",
    "Let's calculate both weights!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e8766a4-611e-49f6-a03d-0cda2ec292fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Linear Weight: [[0.  0.5 1. ]\n",
      " [0.5 0.  0.5]\n",
      " [1.  0.5 0. ]]\n",
      "---\n",
      "- Quadratic Weight: [[0.   0.25 1.  ]\n",
      " [0.25 0.   0.25]\n",
      " [1.   0.25 0.  ]]\n"
     ]
    }
   ],
   "source": [
    "# Linear Weight and Quadratic Weight\n",
    "rows_index = np.array([0, 1, 2])\n",
    "columns_index = np.array([0, 1, 2])\n",
    "normalizer_factor = rows_index.shape[0] - 1\n",
    "\n",
    "linear_weight = np.abs(rows_index[:, None] - columns_index[None, :]) / normalizer_factor\n",
    "quadratic_weight = ((rows_index[:, None] - columns_index[None, :])**2) / normalizer_factor**2\n",
    "\n",
    "print(f'- Linear Weight: {linear_weight}')\n",
    "print('---')\n",
    "print(f'- Quadratic Weight: {quadratic_weight}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca51f13-b438-4305-80a1-e00051ccfd24",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Then we can finally calculate the `Weighted Cohen's Kappa` score using the following equation:\n",
    "\n",
    "```python\n",
    "weighted_cohens_kappa = 1 - (sum(observed_weights * observed_probability)) / (sum(expected_weights * expected_probability))\n",
    "```\n",
    "\n",
    "$$\n",
    "\\text{Weighted Cohen's Kappa} = 1 - \\frac{\\sum_{i}({\\text{observedWeight}_{i} \\cdot \\text{observedProbability}_{i}})}{\\sum_{j}({\\text{expectedWeights}_{j} \\cdot \\text{expectedProbability}_{j}})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55129bee-b192-4e8d-88db-3393253a7af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Linear Weighted Cohen's Kappa: 0.5121951219512195\n"
     ]
    }
   ],
   "source": [
    "# Linear Weight\n",
    "linear_observed_frequency = (linear_weight * observed_frequency).sum()\n",
    "linear_expected_frequency = (linear_weight * expected_frequency).sum()\n",
    "linear_weighted_cohens_kappa = 1 - (linear_observed_frequency / linear_expected_frequency)\n",
    "print(f'- Linear Weighted Cohen\\'s Kappa: {linear_weighted_cohens_kappa}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bd1724f6-3b9e-4be5-a8f2-6106a09921d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Quadratic Weighted Cohen's Kappa: 0.47368421052631593\n"
     ]
    }
   ],
   "source": [
    "# Quadratic Weight\n",
    "quadratic_observed_frequency = (quadratic_weight * observed_frequency).sum()\n",
    "quadratic_expected_frequency = (quadratic_weight * expected_frequency).sum()\n",
    "quadratic_weighted_cohens_kappa = 1 - (quadratic_observed_frequency / quadratic_expected_frequency)\n",
    "print(f'- Quadratic Weighted Cohen\\'s Kappa: {quadratic_weighted_cohens_kappa}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc0db9f-21f5-4be0-8871-48af2c8b2488",
   "metadata": {},
   "source": [
    "Taking our example into consideration, both Linear and Quadratic Weighted Cohen's Kappa score were `Moderate` reliable to the Gold Standard\n",
    "\n",
    "Now let's do the easy way to calculate the score using sklearn!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bb30a1da-a6cf-48ed-a993-109d2e7d678a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Linear Weighted Cohen's Kappa: 0.5121951219512195\n",
      "- Quadratic Weighted Cohen's Kappa: 0.4736842105263158\n"
     ]
    }
   ],
   "source": [
    "# Calculating Weighted Cohen's Kappa Score with SKLean\n",
    "#\n",
    "#  OBS.: the result can differ a few decimals due to roundings\n",
    "#\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "linear_weighted_cohens_kappa = cohen_kappa_score(\n",
    "    gold_standard\n",
    "    , model_predictions\n",
    "    , weights='linear'\n",
    ")\n",
    "\n",
    "quadratic_weighted_cohens_kappa = cohen_kappa_score(\n",
    "    gold_standard\n",
    "    , model_predictions\n",
    "    , weights='quadratic'\n",
    ")\n",
    "\n",
    "print(f'- Linear Weighted Cohen\\'s Kappa: {linear_weighted_cohens_kappa}')\n",
    "print(f'- Quadratic Weighted Cohen\\'s Kappa: {quadratic_weighted_cohens_kappa}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4eeaeb-3ce2-4887-9a5f-771413984aeb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h1 id='reach-me' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>ðŸ“« | Reach Me</h1>\n",
    "\n",
    "> **Email** - [csfelix08@gmail.com](mailto:csfelix08@gmail.com?)\n",
    "\n",
    "> **Linkedin** - [linkedin.com/in/csfelix/](https://www.linkedin.com/in/csfelix/)\n",
    "\n",
    "> **GitHub:** - [CSFelix](https://github.com/CSFelix)\n",
    "\n",
    "> **Kaggle** - [DSFelix](https://www.kaggle.com/dsfelix)\n",
    "\n",
    "> **Portfolio** - [CSFelix.io](https://csfelix.github.io/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
